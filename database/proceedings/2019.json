[
  {
    "title": "Data Usage in MIR: History & Future Recommendations",
    "author": [
      "Wenqin Chen",
      "Jessica Keast",
      "Jordan Moody",
      "Corinne Moriarty",
      "Felicia Villalobos",
      "Virtue Winter",
      "Xueqi Zhang",
      "Xuanqi Lyu",
      "Elizabeth Freeman",
      "Jessie Wang",
      "Sherry Cai",
      "Katherine Kinnaird"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527733",
    "url": "https://doi.org/10.5281/zenodo.3527733",
    "ee": "http://archives.ismir.net/ismir2019/paper/000001.pdf",
    "pages": "25-32",
    "abstract": "The MIR community faces unique challenges in terms of data access, due in large part to country-specific copyright laws. As a result, there is an emerging divide in the MIR research community between labs that have access to music through large companies with abundant funds, and independent labs at smaller institutions who do not have such expansive access. This paper explores how independent researchers have worked to overcome limitations of access to music data without contributing to the crisis of reproducibility.  Acknowledging that there is no single solution for every data access problem that smaller labs face, we propose a number of possibilities for how the MIR community can bridge the gap between advancements from large companies and those within academia. As MIR looks towards the next 20 years, democratizing and expanding access to MIR research and music data is critical. Future solutions could include a  distributed MIREX system, an API designed for MIR researchers, and community-led advocacy to stakeholders.",
    "zenodo_id": 3527733,
    "dblp_key": null,
    "extra": {
      "takeaway": "This paper examines the unique issues of data access that MIR has faced over the last 20 years. We explore datasets used in ISMIR papers, examine the evolution of data access over time, and offer three proposals to increase equity of access to data.",
      "external_links": [],
      "session_position": "A-00"
    }
  },
  {
    "title": "Music Performance Analysis: A Survey",
    "author": [
      "Alexander Lerch",
      "Claire Arthur",
      "Ashis Pati",
      "Siddharth Gururani"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527735",
    "url": "https://doi.org/10.5281/zenodo.3527735",
    "ee": "http://archives.ismir.net/ismir2019/paper/000002.pdf",
    "pages": "33-43",
    "abstract": "Music Information Retrieval (MIR) tends to focus on the analysis of audio signals. Often, a single music recording is used as representative of a \"song\" even though different performances of the same song may reveal different properties. A performance is distinct in many ways from a (arguably more abstract) representation of a \"song,\" \"piece,\" or musical score. The characteristics of the (recorded) performance -as opposed to the score or musical idea- can have a major impact on how a listener perceives music. The analysis of music performance, however, has been traditionally only a peripheral topic for the MIR research community. This paper surveys the field of Music Performance Analysis (MPA) from various perspectives, discusses its significance to the field of MIR, and points out opportunities for future research in this field. ",
    "zenodo_id": 3527735,
    "dblp_key": null,
    "extra": {
      "takeaway": "Music is a performing art. Even so, the performance itself is only infrequently explicitly acknowledged in MIR research. This paper surveys music performance research with the goal of increasing awareness for this topic in the ISMIR community.",
      "external_links": [],
      "session_position": "B-00"
    }
  },
  {
    "title": "Intelligent User Interfaces for Music Discovery: The Past 20 Years and What's to Come",
    "author": [
      "Peter Knees",
      "Markus Schedl",
      "Masataka Goto"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527737",
    "url": "https://doi.org/10.5281/zenodo.3527737",
    "ee": "http://archives.ismir.net/ismir2019/paper/000003.pdf",
    "pages": "44-53",
    "abstract": "Providing means to assist the user in finding music is one of the original motivations underlying the research field known as Music Information Retrieval (MIR). Therefore, already the first edition of ISMIR in the year 2000 called for papers addressing the topic of \"User interfaces for music IR\". Since then, the way humans interact with technology to access and listen to music has substantially changed, not least driven by the advances of MIR and related research fields such as machine learning and recommender systems.   In this paper, we reflect on the evolution of MIR-driven user interfaces for music browsing and discovery over the past two decades. We argue that three major developments have transformed and shaped user interfaces during this period, each connected to a phase of new listening practices: first, connected to personal music collections, intelligent audio processing and content description algorithms that facilitate the automatic organization of repositories and finding music according to sound qualities; second, connected to collective web platforms, the exploitation of user-generated metadata pertaining to semantic descriptions; and third, connected to streaming services, the collection of online music interaction traces on a large scale and their exploitation in recommender systems.  We review and contextualize work from ISMIR and related venues from all three phases and extrapolate current developments to outline possible scenarios of music recommendation and listening interfaces of the future. ",
    "zenodo_id": 3527737,
    "dblp_key": null,
    "extra": {
      "takeaway": "We reflect on the evolution of music discovery interfaces from using content-based analysis, to metadata, to interaction data, while access and listening habits shift from personal collections to streaming services; and extrapolate future trends.",
      "external_links": [],
      "session_position": "C-00"
    }
  },
  {
    "title": "20 Years of Automatic Chord Recognition from Audio",
    "author": [
      "Johan Pauwels",
      "Ken O'Hanlon",
      "Emilia Gomez",
      "Mark B. Sandler"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527739",
    "url": "https://doi.org/10.5281/zenodo.3527739",
    "ee": "http://archives.ismir.net/ismir2019/paper/000004.pdf",
    "pages": "54-63",
    "abstract": "In 1999, Fujishima published \"Realtime Chord Recognition of Musical Sound: a System using Common Lisp Music\". This paper kickstarted an active research topic that has been popular in and around the ISMIR community. The field of Automatic Chord Recognition (ACR) has evolved considerably from early knowledge-based systems towards data-driven methods, with neural network approaches arguably being central to current ACR research. Nonetheless, many of its core issues were already addressed or referred to in the Fujishima paper. In this paper, we review those twenty years of ACR according to these issues. We furthermore attempt to frame current directions in the field in order to establish some perspective for future research.",
    "zenodo_id": 3527739,
    "dblp_key": null,
    "extra": {
      "takeaway": "Looking back on 20 years of automatic chord recognition in order to move forwards",
      "external_links": [],
      "session_position": "E-00"
    }
  },
  {
    "title": "Zero-shot Learning for Audio-based Music Classification and Tagging",
    "author": [
      "Jeong Choi",
      "Jongpil Lee",
      "Jiyoung Park",
      "Juhan Nam"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527741",
    "url": "https://doi.org/10.5281/zenodo.3527741",
    "ee": "http://archives.ismir.net/ismir2019/paper/000005.pdf",
    "pages": "67-74",
    "abstract": "Audio-based music classification and tagging is typically based on categorical supervised learning with a fixed set of labels. This intrinsically cannot handle unseen labels such as newly added music genres or semantic words that users arbitrarily choose for music retrieval. Zero-shot learning can address this problem by leveraging an additional semantic space of labels where auxiliary information about the labels is used to unveil the relationship between each other.  In this work, we investigate the zero-shot learning in music domain and organize two different setups of auxiliary information. One is using human-labeled attribute information based on Free Music Archive and OpenMIC-2018 datasets. The other is using general word semantic information based on Million Song Dataset and Last.fm tag annotations.  Considering a music track is usually multi-labeled in music classification and tagging datasets, we also propose a data split scheme and associated evaluation settings for the multi-label zero-shot learning.  Finally, we report experimental results and discuss the effectiveness and new possibilities of zero-shot learning in music domain. ",
    "zenodo_id": 3527741,
    "dblp_key": null,
    "extra": {
      "takeaway": "Investigated the paradigm of zero-shot learning applied to music domain. Organized 2 side information setups for music calssification task. Proposed a data split scheme and associated evaluation settings for the multi-label zero-shot learning.",
      "external_links": [
        "https://github.com/kunimi00/ZSL_music_tagging"
      ],
      "session_position": "A-01"
    }
  },
  {
    "title": "Learning Notation Graph Construction for Full-Pipeline Optical Music Recognition",
    "author": [
      "Alexander Pacha",
      "Jorge Calvo-Zaragoza",
      "Jan Haji\u010d, jr."
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527744",
    "url": "https://doi.org/10.5281/zenodo.3527744",
    "ee": "http://archives.ismir.net/ismir2019/paper/000006.pdf",
    "pages": "75-82",
    "abstract": "Optical Music Recognition (OMR) promises great benefits to Music Information Retrieval by reducing the costs of making sheet music available in a symbolic format. Recent advances in deep learning have turned typical OMR obstacles into clearly solvable problems, especially the stages that visually process the input image, such as staff line removal or detection of music-notation objects. However, merely detecting objects is not enough for retrieving the actual content, as music notation is a configurational writing system where the semantic of a primitive is defined by its relationship to other primitives. Thus, OMR systems must employ a notation assembly stage to infer such relationships among the detected objects. So far, this stage has been addressed by devising a set of predefined rules or grammars, which hardly generalize well. In this work, we formulate the notation assembly stage from a set of detected primitives as a machine learning problem. Our notation assembly is modeled as a graph that stores syntactic relationships among primitives, which allows us to capture the configuration of symbols in a music-notation document. Our results over the handwritten sheet music corpus MUSCIMA++ show 95.2% precision, 96.0% recall, and an F-score of 95.6% in establishing the correct syntactic relationships. When inferring relationships on data from a music object detector, the model achieves 93.2% precision, 91.5% recall and an F-score of 92.3%.",
    "zenodo_id": 3527744,
    "dblp_key": null,
    "extra": {
      "takeaway": "An Optical Music Recognition system must infer the relationships between detected symbols to understand the semantics of a music score. This notation assembly stage is formulated as a machine learning problem and solved using deep learning.",
      "external_links": [
        "https://github.com/OMR-Research/MungLinker"
      ],
      "session_position": "A-02"
    }
  },
  {
    "title": "An Attention Mechanism for Musical Instrument Recognition",
    "author": [
      "Siddharth Gururani",
      "Mohit Sharma",
      "Alexander Lerch"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527746",
    "url": "https://doi.org/10.5281/zenodo.3527746",
    "ee": "http://archives.ismir.net/ismir2019/paper/000007.pdf",
    "pages": "83-90",
    "abstract": "While the automatic recognition of musical instruments has seen significant progress, the task is still considered hard for music featuring multiple instruments as opposed to single instrument recordings. Datasets for polyphonic instrument recognition can be categorized into roughly two categories. Some, such as MedleyDB, have strong per-frame instrument activity annotations but are usually small in size. Other, larger datasets such as OpenMIC only have weak labels, i.e., instrument presence or absence is annotated only for long snippets of a song. We explore an attention mechanism for handling weakly labeled data for multi-label instrument recognition. Attention has been found to perform well for other tasks with weakly labeled data. We compare the proposed attention model to multiple models which include a baseline binary relevance random forest, recurrent neural network, and fully connected neural networks. Our results show that incorporating attention leads to an overall improvement in classification accuracy metrics across all 20 instruments in the OpenMIC dataset. We find that attention enables models to focus on (or \u2018attend to\u2019) specific time segments in the audio relevant to each instrument label leading to interpretable results.",
    "zenodo_id": 3527746,
    "dblp_key": null,
    "extra": {
      "takeaway": "Instrument recognition in multi-instrument recordings is formulated as a multi-instance multi-label classification problem. We train a model on the weakly labeled OpenMIC dataset using an attention mechanism to aggregate predictions over time.",
      "external_links": [],
      "session_position": "A-03"
    }
  },
  {
    "title": "MIDI-Sheet Music Alignment Using Bootleg Score Synthesis",
    "author": [
      "Thitaree Tanprasert",
      "Teerapat Jenrungrot",
      "Meinard M\u00fcller",
      "Timothy Tsai"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527748",
    "url": "https://doi.org/10.5281/zenodo.3527748",
    "ee": "http://archives.ismir.net/ismir2019/paper/000008.pdf",
    "pages": "91-98",
    "abstract": "MIDI-sheet music alignment is the task of finding correspondences between a MIDI representation of a piece and its corresponding sheet music images.  Rather than using optical music recognition to bridge the gap between sheet music and MIDI, we explore an alternative approach: projecting the MIDI data into pixel space and performing alignment in the image domain.  Our method converts the MIDI data into a crude representation of the score that only contains rectangular floating notehead blobs, a process we call bootleg score synthesis.  Furthermore, we project sheet music images into the same bootleg space by applying a deep watershed notehead detector and filling in the bounding boxes around each detected notehead.  Finally, we align the bootleg representations using a simple variant of dynamic time warping.  On a dataset of 68 real scanned piano scores from IMSLP and corresponding MIDI performances, our method achieves a 97.3% accuracy at an error tolerance of one second, outperforming several baseline systems that employ optical music recognition. ",
    "zenodo_id": 3527748,
    "dblp_key": null,
    "extra": {
      "takeaway": "We propose a mid-level representation called a bootleg score representation which enables alignment between sheet music images and MIDI.",
      "external_links": [],
      "session_position": "A-04"
    }
  },
  {
    "title": "mirdata: Software for Reproducible Usage of Datasets",
    "author": [
      "Rachel Bittner",
      "Magdalena Fuentes",
      "David Rubinstein",
      "Andreas Jansson",
      "Keunwoo Choi",
      "Thor Kell"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527750",
    "url": "https://doi.org/10.5281/zenodo.3527750",
    "ee": "http://archives.ismir.net/ismir2019/paper/000009.pdf",
    "pages": "99-106",
    "abstract": "There are a number of efforts in the MIR community towards increased reproducibility, such as creating more open datasets, publishing code, and the use of common software libraries, e.g. for evaluation. However, when it comes to datasets, there is usually little guarantee that researchers are using the exact same data in the same way, which among other issues, makes comparisons of different methods on the \"same\" datasets problematic.  In this paper, we first show how (often unknown) differences in datasets can lead to significantly different experimental results. We propose a solution to these problems in the form of an open source library, mirdata, which handles datasets in their current distribution modes, but controls for possible variability. In particular, it contains tools which: (1) validate if the user's data (e.g. audio, annotations) is consistent with a canonical version of the dataset; (2) load annotations in a consistent manner; (3) download or give instructions for obtaining data; and (4) make it easy to perform track metadata-specific analysis.",
    "zenodo_id": 3527750,
    "dblp_key": null,
    "extra": {
      "takeaway": "The lack of a standardized way to access and load commonly used datasets is a hurdle towards accelerated and reproducible research. To mitigate this, we present a tool for easy access to data and means to check the integrity of a dataset.",
      "external_links": [
        "https://github.com/mir-dataset-loaders/mirdata"
      ],
      "session_position": "A-05"
    }
  },
  {
    "title": "Cover Detection Using Dominant Melody Embeddings",
    "author": [
      "Guillaume Doras",
      "Geoffroy Peeters"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527752",
    "url": "https://doi.org/10.5281/zenodo.3527752",
    "ee": "http://archives.ismir.net/ismir2019/paper/000010.pdf",
    "pages": "107-114",
    "abstract": "Automatic cover detection -- the task of finding in an audio database all the covers of one or several query tracks -- has long been seen as a challenging theoretical problem in the MIR community and as an acute practical problem for authors and composers societies. Original algorithms proposed for this task have proven their accuracy on small datasets, but are unable to scale up to modern real-life audio corpora. On the other hand, faster approaches designed to process thousands of pairwise comparisons resulted in lower accuracy, making them unsuitable for practical use.  In this work, we propose a neural network architecture that is trained to represent each track as a single embedding vector. The computation burden is therefore left to the embedding extraction -- that can be conducted offline and stored, while the pairwise comparison task reduces to a simple Euclidean distance computation. We further propose to extract each track's embedding out of its dominant melody representation, obtained by another neural network trained for this task. We then show that this architecture improves state-of-the-art accuracy both on small and large datasets, and is able to scale to query databases of thousands of tracks in a few seconds.",
    "zenodo_id": 3527752,
    "dblp_key": null,
    "extra": {
      "takeaway": "We propose a cover detection method based on vector embedding extraction out of audio dominant melody. This architecture improves state-of-the-art accuracy on large datasets, and scales to query collections of thousands of tracks in a few seconds.",
      "external_links": [],
      "session_position": "A-06"
    }
  },
  {
    "title": "Identifying Expressive Semantics in Orchestral Conducting Kinematics",
    "author": [
      "Yu-Fen Huang",
      "Tsung-Ping Chen",
      "Nikki Moran",
      "Simon Coleman",
      "Li Su"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527754",
    "url": "https://doi.org/10.5281/zenodo.3527754",
    "ee": "http://archives.ismir.net/ismir2019/paper/000011.pdf",
    "pages": "115-122",
    "abstract": "Existing kinematic research on orchestral conducting movement contributes to beat-tracking and the delivery of performance dynamics. Methodologically, such movement cues have been treated as distinct, isolated events. Yet as practicing musicians and music pedagogues know, conductors\u2019 expressive instructions are highly flexible and dependent on the musical context. We seek to demonstrate an approach to search for effective descriptors to express musical features in conducting movement in a valid music context, and to extract complex expressive semantics from elementary conducting kinematic variations. This study therefore proposes a multi-task learning model to jointly identify dynamic, articulation, and phrasing cues from conducting kinematics. A professional conducting movement dataset is compiled using a high-resolution motion capture system. The ReliefF algorithm is applied to select significant features from conducting movement, and recurrent neural network (RNN) is implemented to identify multiple movement cues. The experimental results disclose key elements in conducting movement which communicate musical expressiveness; the results also highlight the advantage of multi-task learning in the complete musical context over single-task learning. To the best of our knowledge, this is the first attempt to use recurrent neural network to explore multiple semantic expressive cuing in conducting movement kinematics.",
    "zenodo_id": 3527754,
    "dblp_key": null,
    "extra": {
      "takeaway": "As the pioneering investigation on conducting movement using RNN, we highlight the potential for this framework to be applied to further explore other issues in music conducting.",
      "external_links": [],
      "session_position": "A-07"
    }
  },
  {
    "title": "The RomanText Format: A Flexible and Standard Method for Representing Roman Numerial Analyses",
    "author": [
      "Mark Gotham",
      "Dmitri Tymoczko",
      "Michael Cuthbert"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527756",
    "url": "https://doi.org/10.5281/zenodo.3527756",
    "ee": "http://archives.ismir.net/ismir2019/paper/000012.pdf",
    "pages": "123-129",
    "abstract": "Roman numeral analysis has been central to the Western musician\u2019s toolkit since its emergence in the early nineteenth century: it is an extremely popular method for recording subjective analytical decisions about the chords and keys implied by a passage of music. Disagreements about these judgments have led to extensive theoretical debates and ongoing controversies. Such debates are exacerbated by the absence of a pubic corpus of expert Roman numeral analyses, and by the more fundamental lack of an agreed-upon, computer-readable syntax in which those analyses might be expressed. This paper specifies such a standard, along with an associated code library in music21, and a preliminary set of example corpora. To frame the project, we review some of the motivations for doing harmonic analysis, some reasons why it resists automation, and some prospective uses for our tools.",
    "zenodo_id": 3527756,
    "dblp_key": null,
    "extra": {
      "takeaway": "We provide a technical standard, converter code and example corpora for Roman-numeral analysis, enabling a range of computational, musical, and pedagogical use cases.",
      "external_links": [
        "http://web.mit.edu/music21/doc/moduleReference/moduleRoman.html",
        "https://github.com/MarkGotham/When-in-Rome"
      ],
      "session_position": "A-08"
    }
  },
  {
    "title": "20 Years of Playlists: A Statistical Analysis on Popularity and Diversity",
    "author": [
      "Lorenzo Porcaro",
      "Emilia Gomez"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527758",
    "url": "https://doi.org/10.5281/zenodo.3527758",
    "ee": "http://archives.ismir.net/ismir2019/paper/000013.pdf",
    "pages": "130-136",
    "abstract": "Grouping songs together, according to music preferences, mood or other characteristics, is an activity which reflects personal listening behaviours and tastes. In the last two decades, due to the increasing size of music catalogues accessible and to improvements of recommendation algorithms, people have been exposed to new ways for creating playlists. In this work, through the statistical analysis of more than 400K playlists from four datasets, created in different temporal and technological contexts, we aim to understand if it is possible to extract information about the evolution of humans strategies for playlist creation. We focus our analysis on two driving concepts of the Music Information Retrieval literature: popularity and diversity.",
    "zenodo_id": 3527758,
    "dblp_key": null,
    "extra": {
      "takeaway": "We find extremely valuable to compare playlist datasets generated in different contexts, as it allows to understand how changes in the listening experience are affecting playlist creation strategies.",
      "external_links": [
        "https://github.com/MTG/playlists-stat-analysis"
      ],
      "session_position": "A-09"
    }
  },
  {
    "title": "Identification and Cross-Document Alignment of Measures in Music Score Images",
    "author": [
      "Simon Waloschek",
      "Aristotelis Hadjakos",
      "Alexander Pacha"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527760",
    "url": "https://doi.org/10.5281/zenodo.3527760",
    "ee": "http://archives.ismir.net/ismir2019/paper/000014.pdf",
    "pages": "137-143",
    "abstract": "In the course of editing musical works, musicologists regularly compare multiple sources of the same musical piece, such as composers' autographs, handwritten copies, and various prints. For efficient comparison, cross-source navigation is essential, enabling to quickly jump back and forth between multiple sources without losing the current musical position. In practice, measures are first annotated by hand in the individual source images and then related to each other. Our approach automates this time-consuming and error-prone process with the help of deep learning. For this purpose, we train a neural network that automatically finds bounding boxes of all measures in images. A second network is trained to compute the similarity between two measures to determine if they have the same musical content and should, therefore, be linked for navigation. Sequences of outputs from the second network are matched using Dynamic Time Warping to provide the final proposal of measure relationships, so-called concordances. In addition to cross-source navigation, the results can be used to spot structural differences across the sources which are essential for editorial work, so that musicologists can focus more on analytical tasks.",
    "zenodo_id": 3527760,
    "dblp_key": null,
    "extra": {
      "takeaway": "Musicologists regularly compare multiple sources of the same musical piece. To enable cross-source navigation in music score image, we propose a machine-learning approach which automatically detects and aligns measures across multiple sources.",
      "external_links": [
        "https://github.com/cemfi/measure-detector"
      ],
      "session_position": "A-10"
    }
  },
  {
    "title": "Query-by-Blending: A Music Exploration System Blending Latent Vector Representations of Lyric Word, Song Audio, and Artist",
    "author": [
      "Kento Watanabe",
      "Masataka Goto"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527762",
    "url": "https://doi.org/10.5281/zenodo.3527762",
    "ee": "http://archives.ismir.net/ismir2019/paper/000015.pdf",
    "pages": "144-151",
    "abstract": "This paper presents Query-by-Blending, a novel music exploration system that enables users to find unfamiliar music content by flexibly combining three musical aspects: lyric word, song audio, and artist. Although there are various systems for music retrieval based on the similarity between songs or artists and for music browsing based on visualized songs, it is still difficult to explore unfamiliar content by flexibly combining multiple musical aspects. Query-by-Blending overcomes this difficulty by representing each of the aspects as a latent vector representation (called a \"flavor\" in this paper) that is a distinctive quality felt to be characteristic of a given word/song/artist. By giving a lyric word as a query, for example, a user can find songs and artists whose flavors are similar to the flavor of the query word.  Moreover, by giving a query combining (blending) lyric-word and song-audio flavors, the user can interactively explore unfamiliar content containing the blended flavor. This multi-aspect blending was achieved by constructing a novel vector space model into which all of the lyric words, song audio tracks, and artist IDs of a collection can be embedded. In our experiments, we embedded 14,505 lyric words, 433,936 songs, and 44,696 artists into the same shared vector space and found that the system can appropriately calculate similarities between different aspects and blend flavors to find related lyric words, songs, and artists.",
    "zenodo_id": 3527762,
    "dblp_key": null,
    "extra": {
      "takeaway": "Query-by-Blending is a music exploration system that lets users find music by combining three musical aspects: lyric word, song audio, and artist. We propose an embedding method of constructing a unified vector space by using unsupervised learning.",
      "external_links": [],
      "session_position": "A-11"
    }
  },
  {
    "title": "Improving Structure Evaluation Through Automatic Hierarchy Expansion",
    "author": [
      "Brian McFee",
      "Katherine Kinnaird"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527764",
    "url": "https://doi.org/10.5281/zenodo.3527764",
    "ee": "http://archives.ismir.net/ismir2019/paper/000016.pdf",
    "pages": "152-158",
    "abstract": "Structural segmentation is the task of partitioning a recording into non-overlapping time intervals, and labeling each segment with an identifying marker such as A, B, or verse. Hierarchical structure annotation expands this idea to allow an annotator to segment a song with multiple levels of granularity. While there has been recent progress in developing evaluation criteria for comparing two hierarchical annotations of the same recording, the existing methods have known deficiencies when dealing with inexact label matchings and sequential label repetition.  In this article, we investigate methods for automatically enhancing structural annotations by inferring (and expanding) hierarchical information from the segment labels. The proposed method complements existing techniques for comparing hierarchical structural annotations by coarsening or refining labels with variation markers to either collapse similarly labeled segments together, or separate identically labeled segments from each other. Using the multi-level structure annotations provided in the SALAMI dataset, we demonstrate that automatic hierarchy expansion allows structure comparison methods to more accurately assess similarity between annotations.",
    "zenodo_id": 3527764,
    "dblp_key": null,
    "extra": {
      "takeaway": "We propose a method to expose latent hierarchical content in structural segmentation labels. This results in more accurate comparisons between multi-level segmentations. ",
      "external_links": [],
      "session_position": "A-12"
    }
  },
  {
    "title": "Conditioned-U-Net: Introducing a Control Mechanism in the U-Net for Multiple Source Separations",
    "author": [
      "Gabriel Meseguer Brocal",
      "Geoffroy Peeters"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527766",
    "url": "https://doi.org/10.5281/zenodo.3527766",
    "ee": "http://archives.ismir.net/ismir2019/paper/000017.pdf",
    "pages": "159-165",
    "abstract": "Data-driven models for audio source separation such as U-Net or Wave-U-Net are usually models dedicated to and specifically trained for a single task, e.g. a particular instrument isolation. Training them for various tasks at once commonly results in worse performances than training them for a single specialized task. In this work, we introduce the Conditioned-U-Net (C-U-Net) which adds a control mechanism to the standard U-Net. The control mechanism allows us to train a unique and generic U-Net to perform the separation of various instruments. The C-U-Net decides the instrument to isolate according to a one-hot-encoding input vector. The input vector is embedded to obtain the parameters that control Feature-wise Linear Modulation (FiLM) layers. FiLM layers modify the U-Net feature maps in order to separate the desired instrument via affine transformations. The C-U-Net performs different instrument separations, all with a single model achieving the same performances as the dedicated ones at a lower cost.",
    "zenodo_id": 3527766,
    "dblp_key": null,
    "extra": {
      "takeaway": "In this paper, we apply conditioning learning to source separation and introduce a control mechanism to the standard U-Net architecture. The control mechanism allows multiple instrument separations with just one model without losing performance.",
      "external_links": [
        "https://github.com/gabolsgabs/cunet"
      ],
      "session_position": "A-13"
    }
  },
  {
    "title": "An Initial Computational Model for Musical Schemata Theory",
    "author": [
      "Andreas Katsiavalos",
      "Tom Collins",
      "Bret Battey"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527768",
    "url": "https://doi.org/10.5281/zenodo.3527768",
    "ee": "http://archives.ismir.net/ismir2019/paper/000018.pdf",
    "pages": "166-172",
    "abstract": "Musical schemata theory entails the classification of subphrase-length progressions in melodic, harmonic and metric feature-sets as named entities (e.g., `Romanesca', `Meyer', `Cadence', etc.), where a musical schema is characterized by factors such as music content and form, position and tonal function within phrase structure, and interrelation with other schemata. To examine and automate the task of musical schemata classification, we developed a novel musical schemata classifier. First, we tested methods for exact and approximate matching of user-defined schemata prototypes, to establish the notions of identity and similarity between composite music patterns. Next, we examined methods for schemata prototype extraction from collections of same-labelled annotated examples, performing training and testing sessions similar to supervised learning approaches. The performance of the above tasks was verified using the same annotated dataset of 40 keyboard sonata excerpts from pre-Classical and Classical periods. Our evaluation of the classifier sheds light on: (a)~ability to parse and interpret music information, (b)~similarity methods for composite music patterns, (c)~categorization methods for polyphonic music.",
    "zenodo_id": 3527768,
    "dblp_key": null,
    "extra": {
      "takeaway": "This paper presents a novel classifier for short polyphonic passages in Classical works that performs musical schemata recognition and prototype extraction with the utilisation of high-level musical constructs and similarity functions.",
      "external_links": [
        "http://tomcollinsresearch.net/research/schemata/"
      ],
      "session_position": "A-14"
    }
  },
  {
    "title": "Evolution of the Informational Complexity of Contemporary Western Music",
    "author": [
      "Thomas Parmer",
      "Yong-Yeol Ahn"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527772",
    "url": "https://doi.org/10.5281/zenodo.3527772",
    "ee": "http://archives.ismir.net/ismir2019/paper/000019.pdf",
    "pages": "175-182",
    "abstract": "We measure the complexity of songs in the Million Song Dataset (MSD) in terms of pitch, timbre, loudness, and rhythm to investigate their evolution from 1960 to 2010. By comparing the Billboard Hot 100 with random samples, we find that the complexity of popular songs tends to be more narrowly distributed around the mean, supporting the idea of an inverted U-shaped relationship between complexity and hedonistic value. We then examine the temporal evolution of complexity, reporting consistent changes across decades, such as a decrease in average loudness complexity since the 1960s, and an increase in timbre complexity overall but not for popular songs. We also show, in contrast to claims that popular songs sound more alike over time, that they are not more similar than they were 50 years ago in terms of pitch or rhythm, although similarity in timbre shows distinctive patterns across eras and similarity in loudness has been increasing. Finally, we show that musical genres can be differentiated by their distinctive complexity profiles.",
    "zenodo_id": 3527772,
    "dblp_key": null,
    "extra": {
      "takeaway": "We find evidence for a global, inverted U-shaped relationship between complexity and hedonistic value within Western contemporary music, suggesting that the most popular songs cluster around average complexity values.",
      "external_links": [],
      "session_position": "B-01"
    }
  },
  {
    "title": "Deep Unsupervised Drum Transcription",
    "author": [
      "Keunwoo Choi",
      "Kyunghyun Cho"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527774",
    "url": "https://doi.org/10.5281/zenodo.3527774",
    "ee": "http://archives.ismir.net/ismir2019/paper/000020.pdf",
    "pages": "183-191",
    "abstract": "We introduce DrummerNet, a drum transcription system that is trained in an unsupervised manner. DrummerNet does not require any ground-truth transcription and, with the data-scalability of deep neural networks, learns from a large unlabeled dataset. In DrummerNet, the target drum signal is first passed to a (trainable) transcriber, then reconstructed in a (fixed) synthesizer according to the transcription estimate. By training the system to minimize the distance between the input and the output audio signals, the transcriber learns to transcribe without ground truth transcription. Our experiment shows that DrummerNet performs favorably compared to many other recent drum transcription systems, both supervised and unsupervised.",
    "zenodo_id": 3527774,
    "dblp_key": null,
    "extra": {
      "takeaway": "DrummerNet is a drum transcriber trained in an unsupervised fashion. DrummerNet learns to transcribe by learning to reconstruct the audio with the transcription estimate. Unsupervised learning + a large dataset allow DrummerNet to be less-biased.",
      "external_links": [
        "https://github.com/keunwoochoi/DrummerNet"
      ],
      "session_position": "B-02"
    }
  },
  {
    "title": "Estimating Unobserved Audio Features for Target-Based Orchestration",
    "author": [
      "Jon Gillick",
      "Carmine-Emanuele Cella",
      "David Bamman"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527776",
    "url": "https://doi.org/10.5281/zenodo.3527776",
    "ee": "http://archives.ismir.net/ismir2019/paper/000021.pdf",
    "pages": "192-199",
    "abstract": "Target-based assisted orchestration can be thought of as the process of searching for optimal combinations of sounds to match a target sound, given a database of samples, a similarity metric, and a set of constraints.  A typical solution to this problem is a proposed orchestral score where candidate scores are ranked by similarity in some feature space between the target sound and the mixture of audio samples in the database corresponding to the notes in the score; in the orchestral setting, valid scores may contain dozens of instruments sounding simultaneously.  Generally, target-based assisted orchestration systems consist of a combinatorial optimization algorithm and a constraint solver that are jointly optimized to find valid solutions.  A key step in the optimization involves generating a large number of combinations of sounds from the database and then comparing the features of each mixture of sounds with the target sound.  Because of the high computational cost required to synthesize a new audio file and then compute features for every combination of sounds, in practice, existing systems instead estimate the features of each new mixture using precomputed features of the individual source files making up the combination.  Currently, state of the art systems use a simple linear combination to make these predictions, even if the features in use are not themselves linear.  In this work, we explore neural models for estimating the features of a mixture of sounds from the features of the component sounds, finding that standard features can be estimated with accuracy significantly better than that of the methods currently used in assisted orchestration systems.  We present quantitative comparisons and discuss the implications of our findings for target-based orchestration problems.",
    "zenodo_id": 3527776,
    "dblp_key": null,
    "extra": {
      "takeaway": "We show that neural networks can predict features of the sum of 30 or more individual music notes based only on precomputed features of the source notes. This holds promise for computationally expensive applications like target-based orchestration.",
      "external_links": [
        "https://github.com/jrgillick/audio-feature-forecasting"
      ],
      "session_position": "B-03"
    }
  },
  {
    "title": "Towards Automatically Correcting Tapped Beat Annotations for Music Recordings",
    "author": [
      "Jonathan Driedger",
      "Hendrik Schreiber",
      "Bas de Haas",
      "Meinard M\u00fcller"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527778",
    "url": "https://doi.org/10.5281/zenodo.3527778",
    "ee": "http://archives.ismir.net/ismir2019/paper/000022.pdf",
    "pages": "200-207",
    "abstract": "A common method to create beat annotations for music recordings is to let a human annotator tap along with them. However, this method is problematic due to the limited human ability to temporally align taps with audio cues for beats accurately. In order to create accurate beat annotations, it is therefore typically necessary to manually correct the recorded taps in a subsequent step, which is a cumbersome task. In this work we aim to automate this correction step by \"snapping\" the taps to close-by audio cues - a strategy that is often used by beat tracking algorithms to refine their beat estimates. The main contributions of this paper can be summarized as follows. First, we formalize the automated correction procedure mathematically. Second, we introduce a novel visualization method that serves as a tool to analyze the results of the correction procedure for potential errors. Third, we present a new dataset consisting of beat annotations for 101 music recordings. Fourth, we use this dataset to perform a listening experiment as well as a quantitative study to show the effectiveness of our snapping procedure. ",
    "zenodo_id": 3527778,
    "dblp_key": null,
    "extra": {
      "takeaway": "A framework for correcting beat annotations that were created by humans tapping to the beat of music recordings. It includes an automated correction procedure, visualizations to inspect the correction process, and a new dataset of beat annotations.",
      "external_links": [
        "https://github.com/chordify/tapcorrect"
      ],
      "session_position": "B-04"
    }
  },
  {
    "title": "Algorithmic Ability to Predict the Musical Future: Datasets and Evaluation",
    "author": [
      "Berit Janssen",
      "Tom Collins",
      "Iris Yuping Ren"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527780",
    "url": "https://doi.org/10.5281/zenodo.3527780",
    "ee": "http://archives.ismir.net/ismir2019/paper/000023.pdf",
    "pages": "208-215",
    "abstract": "Music prediction and generation have been of recurring interest in the field of music informatics: many models that emulate listeners' musical expectancies, or that produce novel musical content have been introduced over the past few decades. So far, these models have mostly been evaluated in isolation, following diverse evaluation strategies. Our paper provides an overview of the new MIREX task Patterns for Prediction. We introduce a dataset, which contains monophonic and polyphonic data, both in symbolic and audio representations. We suggest a standardized evaluation procedure to compare algorithmic musical predictions. We compare two neural network models to a baseline model and show that algorithmic approaches can correctly predict about a third of a monophonic segment, and around half of a polyphonic segment, with one of the neural network models achieving best results. However, other approaches to algorithmic music prediction are needed to achieve a more rounded picture of the potential of state-of-the-art methods of music prediction.",
    "zenodo_id": 3527780,
    "dblp_key": null,
    "extra": {
      "takeaway": "We introduce a dataset and evaluation methods to compare music prediction models, used in the MIREX Patterns for Prediction task. We compare three models in our framework, and discuss how to improve evaluation strategies and music prediction models.",
      "external_links": [
        "https://tinyurl.com/y455cf97"
      ],
      "session_position": "B-05"
    }
  },
  {
    "title": "Learning Soft-Attention Models for Tempo-invariant Audio-Sheet Music Retrieval",
    "author": [
      "Stefan Balke",
      "Matthias Dorfer",
      "Luis Carvalho",
      "Andreas Arzt",
      "Gerhard Widmer"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527782",
    "url": "https://doi.org/10.5281/zenodo.3527782",
    "ee": "http://archives.ismir.net/ismir2019/paper/000024.pdf",
    "pages": "216-222",
    "abstract": "Connecting large libraries of digitized audio recordings to their corresponding sheet music images has long been a motivation for researchers to develop new cross-modal retrieval systems. In recent years, retrieval systems based on embedding space learning with deep neural networks got a step closer to fulfilling this vision. However, global and local tempo deviations in the music recordings still require careful tuning of the amount of temporal context given to the system. In this paper, we address this problem by introducing an additional soft-attention mechanism on the audio input. Quantitative and qualitative results on synthesized piano data indicate that this attention increases the robustness of the retrieval system by focusing on different parts of the input representation based on the tempo of the audio. Encouraged by these results, we argue for the potential of attention models as a very general tool for many MIR tasks.",
    "zenodo_id": 3527782,
    "dblp_key": null,
    "extra": {
      "takeaway": "The amount of temporal context given to a CNN is adapted by an additional soft-attention network, enabling the network to react to local and global tempo deviations in the input audio spectrogram.",
      "external_links": [
        "http://www.cp.jku.at/resources/2019_ASR-TempoInv_ISMIR/"
      ],
      "session_position": "B-06"
    }
  },
  {
    "title": "Contributing to New Musicological Theories with Computational Methods: The Case of Centonization in Arab-Andalusian Music",
    "author": [
      "Thomas Nuttall",
      "Miguel Garc\u00eda-Casado",
      "V\u00edctor N\u00fa\u00f1ez-Tarifa",
      "Rafael Caro Repetto",
      "Xavier Serra"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527784",
    "url": "https://doi.org/10.5281/zenodo.3527784",
    "ee": "http://archives.ismir.net/ismir2019/paper/000025.pdf",
    "pages": "223-228",
    "abstract": "Arab-Andalusian music was formed in the medieval Islamic territories of Iberian Peninsula, drawing on local traditions and assuming Arabic influences. The expert performer and researcher of the Moroccan tradition of this music, Amin Chaachoo, is developing a theory, whose last formulation was recently published in La Mu-sique Hispano-Arabe, al-Ala (2016), which argues that centonization, a melodic composition technique used in Gregorian chant, was also utilized for the creation of this repertoire. In this paper we aim to contribute to Chaachoo\u2019s theory by means of tf-idf analysis. A highorder n-gram model is applied to a corpus of 149 prescriptive transcriptions of heterophonic recordings, representing each as an unordered multiset of patterns. Computing the tf-idf statistic of each pattern in this corpus provides a means by which we can rank and compare motivic content across nawab\u0101t, distinct musical forms of the tradition. For each nawba, an empirical comparison is made between patterns identified as significant via our approach and those proposed by Chaachoo. Ultimately we observe considerable agreement between the two pattern sets and go further in proposing new, unique and as yet undocumented patterns that occur at least as frequently and with at least as much importance as those in Chaachoo\u2019s proposals.",
    "zenodo_id": 3527784,
    "dblp_key": null,
    "extra": {
      "takeaway": "Here we demonstrate how relatively uncomplicated statistical methods can support and contribute to new musicological theory, namely that developed by expert performer and researcher of Arab-Andalusian music of the Moroccan tradition, Amin Chaachoo.",
      "external_links": [],
      "session_position": "B-07"
    }
  },
  {
    "title": "Temporal Convolutional Networks for Speech and Music Detection in Radio Broadcast",
    "author": [
      "Quentin Lemaire",
      "Andre Holzapfel"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527786",
    "url": "https://doi.org/10.5281/zenodo.3527786",
    "ee": "http://archives.ismir.net/ismir2019/paper/000026.pdf",
    "pages": "229-236",
    "abstract": " The task of speech and music detection aims at the automatic annotation of potentially overlapping speech and music segments in audio recordings. This meta-data extraction process has important applications in royalty collection in broadcast audio. This study focuses on  deep neural network architectures made to process sequential data, and a series of recent architectures that have not yet been applied for this task are evaluated, extended and compared with a state-of-the-art architecture. Moreover, different training strategies are evaluated, and we demonstrate the advantages of a step-wise procedure that facilitates the combination heterogeneous datasets. The study shows that Temporal Convolution Network (TCN) architectures can outperform state-of-the-art architectures, and that especially the novel extension of non-causal TCN introduced in this paper leads to a significant improvement in the accuracy.",
    "zenodo_id": 3527786,
    "dblp_key": null,
    "extra": {
      "takeaway": "This study shows that a novel deep neural network architecture for sequential data (non-causal Temporal Convolution Network) can outperform state-of-the-art architectures in the task of speech and music detection.",
      "external_links": [
        "https://github.com/qlemaire22/speech-music-detection"
      ],
      "session_position": "B-08"
    }
  },
  {
    "title": "Towards Explainable Music Emotion Recognition: The Route via Mid-level Features",
    "author": [
      "Shreyan Chowdhury",
      "Andreu Vall Portabella",
      "Verena Haunschmid",
      "Gerhard Widmer"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527788",
    "url": "https://doi.org/10.5281/zenodo.3527788",
    "ee": "http://archives.ismir.net/ismir2019/paper/000027.pdf",
    "pages": "237-243",
    "abstract": "Emotional aspects play an important part in our interaction with music. However, modeling this aspect in MIR systems has been notoriously challenging since emotion is an inherently abstract and subjective experience, thus making it difficult to quantify or predict in the first place, and to make sense of the predictions in the next. In an attempt to create a model that can give a musically meaningful and intuitive explanation for its prediction, we propose a VGG-style deep neural network that learns to predict emotional characteristics of a musical piece together with (and based on) human-interpretable, mid-level perceptual features. We compare this to predicting emotion directly with an identical network that does not take into account the mid-level features, and observe that the cost of going through the mid-level features is surprisingly low, on average. The design of our network allows us to visualize the effects of perceptual features on individual emotion predictions, and we argue that the small loss in performance in going through the mid-level features is justified by the gain in explainability of the predictions.",
    "zenodo_id": 3527788,
    "dblp_key": null,
    "extra": {
      "takeaway": "Explainable predictions of emotion from music can be obtained by introducing an intermediate representation of mid-level perceptual features in the predictor deep neural network.",
      "external_links": [
        "https://shreyanc.github.io/ismir_example.html"
      ],
      "session_position": "B-09"
    }
  },
  {
    "title": "Community-Based Cover Song Detection",
    "author": [
      "Jonathan Donier"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527790",
    "url": "https://doi.org/10.5281/zenodo.3527790",
    "ee": "http://archives.ismir.net/ismir2019/paper/000028.pdf",
    "pages": "244-250",
    "abstract": "Audio-based cover song detection has received much attention in the MIR community in the recent years. To date, the most popular formulation of the problem has been to compare the audio signals of two tracks and to make a binary decision based on this information only. However, leveraging additional signals might be key if one wants to solve the problem at an industrial scale. In this paper, we introduce an ensemble-based method that approaches the problem from a many-to-many perspective.  Instead of considering pairs of tracks in isolation, we consider larger sets of potential versions for a given composition, and create and exploit the graph of relationships between these tracks. We show that this can result in a significant improvement in performance, in particular when the number of existing versions of a given composition is large.",
    "zenodo_id": 3527790,
    "dblp_key": null,
    "extra": {
      "takeaway": "We approach cover song detection by considering larger sets of potential versions for a given work, and create and exploit the graph of relationships between these versions. We show a significant improvement in performance over a 1-vs-1 method.",
      "external_links": [],
      "session_position": "B-10"
    }
  },
  {
    "title": "Tracking Beats and Microtiming in Afro-Latin American Music Using Conditional Random Fields and Deep Learning",
    "author": [
      "Magdalena Fuentes",
      "Lucas Maia",
      "Mart\u00edn Rocamora",
      "Luiz Biscainho",
      "Helene-Camille Crayencour",
      "Slim Essid",
      "Juan Bello"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527792",
    "url": "https://doi.org/10.5281/zenodo.3527792",
    "ee": "http://archives.ismir.net/ismir2019/paper/000029.pdf",
    "pages": "251-258",
    "abstract": "Events in music frequently exhibit small-scale temporal deviations (microtiming), with respect to the underlying regular metrical grid. In some cases, as in music from the Afro-Latin American tradition, such deviations appear systematically, disclosing their structural importance in rhythmic and stylistic configuration. In this work we explore the idea of automatically and jointly tracking beats and microtiming in timekeeper instruments of Afro-Latin American music, in particular Brazilian samba and Uruguayan candombe. To that end, we propose a language model based on conditional random fields that integrates beat and onset likelihoods as observations. We derive those activations using deep neural networks and evaluate its performance on manually annotated data using a scheme adapted to this task. We assess our approach in controlled conditions suitable for these timekeeper instruments, and study the microtiming profiles' dependency on genre and performer, illustrating promising aspects of this technique towards a more comprehensive understanding of these music traditions.",
    "zenodo_id": 3527792,
    "dblp_key": null,
    "extra": {
      "takeaway": "A CRF model is able to automatically and jointly track beats and microtiming in timekeeper instruments of Afro-Latin American music, in particular samba and candombe. This allows the study of microtiming profiles' dependency on genre and performer.",
      "external_links": [],
      "session_position": "B-11"
    }
  },
  {
    "title": "Harmony Transformer: Incorporating Chord Segmentation into Harmony Recognition",
    "author": [
      "Tsung-Ping Chen",
      "Li Su"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527794",
    "url": "https://doi.org/10.5281/zenodo.3527794",
    "ee": "http://archives.ismir.net/ismir2019/paper/000030.pdf",
    "pages": "259-267",
    "abstract": "Musical harmony analysis is usually a process of unfolding and interpreting the hierarchical structure of music. Computational approaches to such structural analysis are still challenging, owing to the fact that the boundary between different harmonic states (such as chord functions) is not explicitly defined in the audio or symbolic music data. It is a novel approach to improve chord recognition by jointly identifying chord change using end-to-end sequence learning. In this paper, we propose the Harmony Transformer, a multi-task music harmony analysis model aiming to improve chord recognition through incorporating chord segmentation into the recognition process. The integration of chord segmentation and chord recognition is implemented with the Transformer, a deep sequential learning model yielding fruitful results in the field of natural language processing. A non-autoregressive decoding framework is also adopted here in aid of concatenating the two highly correlated tasks. Experiments of both chord symbol recognition and functional harmony recognition on audio and symbolic datasets demonstrate that explicitly learning the hierarchical structural information of musical data can facilitate and improve the harmony recognition.",
    "zenodo_id": 3527794,
    "dblp_key": null,
    "extra": {
      "takeaway": "Incorporating chord segmentation into chord recognition using the Transformer model achieves improved performance over prior art.",
      "external_links": [],
      "session_position": "B-12"
    }
  },
  {
    "title": "Statistical Music Structure Analysis Based on a Homogeneity-, Repetitiveness-, and Regularity-Aware Hierarchical Hidden Semi-Markov Model",
    "author": [
      "Go Shibata",
      "Ryo Nishikimi",
      "Eita Nakamura",
      "Kazuyoshi Yoshii"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527796",
    "url": "https://doi.org/10.5281/zenodo.3527796",
    "ee": "http://archives.ismir.net/ismir2019/paper/000031.pdf",
    "pages": "268-275",
    "abstract": "This paper describes a music structure analysis method that splits music audio signals into meaningful segments such as musical sections and clusters them. In this task, how to model the four fundamental aspects of musical sections, i.e., homogeneity, repetitiveness, novelty, and regularity, in a unified way is still an open problem. Here we propose a solid statistical approach based on a homogeneity-, repetitiveness-, and regularity-aware hierarchical hidden semi-Markov model. The higher-level semi-Markov chain represents a sequence of sections that tend to have regularly spaced boundaries. The timbral features in each section are assumed to follow emission distributions that are homogeneous over time. The lower-level left-to-right Markov chain in each section represents a chord sequence whose sequential order is constrained to be a repetition of a chord sequence in another section of the same cluster. The whole model can be trained unsupervisedly based on Bayesian sparse learning where unnecessary sections automatically degenerate. The proposed method outperformed representative methods in segmentation and clustering accuracies with estimated sections having similar statistical properties as the ground truth data.",
    "zenodo_id": 3527796,
    "dblp_key": null,
    "extra": {
      "takeaway": "This paper proposes a solid statistical approach to music structure analysis based on a homogeneity-, repetitiveness-, and regularity-aware hierarchical hidden semi-Markov model.",
      "external_links": [],
      "session_position": "B-13"
    }
  },
  {
    "title": "Towards Measuring Intonation Quality of Choir Recordings: A Case Study on Bruckner's Locus Iste",
    "author": [
      "Christof Weiss",
      "Sebastian J. Schlecht",
      "Sebastian Rosenzweig",
      "Meinard M\u00fcller"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527798",
    "url": "https://doi.org/10.5281/zenodo.3527798",
    "ee": "http://archives.ismir.net/ismir2019/paper/000032.pdf",
    "pages": "276-283",
    "abstract": "Unaccompanied vocal music is a central part of Western art music, yet it requires excellent skills for singers to achieve proper intonation. In this paper, we analyze intonation deficiencies by introducing an intonation cost measure that can be computed from choir recordings and may help to assess the singers' intonation quality. With our approach, we measure the deviation between the recording's local salient frequency content and an adaptive reference grid based on the equal-tempered scale. The adaptivity introduces invariance of the local intonation measure to global intonation drifts. In our experiments, we compute this measure for several recordings of Anton Bruckner's choir piece Locus Iste. We demonstrate the robustness of the proposed measure by comparing scenarios of different complexity regarding the availability of aligned scores and multi-track recordings, as well as the number of singers per part. Even without using score information, our cost measure shows interesting trends, thus indicating the potential of our method for real-world applications.",
    "zenodo_id": 3527798,
    "dblp_key": null,
    "extra": {
      "takeaway": "This paper proposes an intonation cost measure for assessing the intonation quality of choir singing. While capturing local frequency deviations, the measure includes a grid shift compensation for cases when the entire choir is drifting in pitch.",
      "external_links": [],
      "session_position": "B-14"
    }
  },
  {
    "title": "Guitar Tablature Estimation with a Convolutional Neural Network",
    "author": [
      "Andrew Wiggins",
      "Youngmoo Kim"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527800",
    "url": "https://doi.org/10.5281/zenodo.3527800",
    "ee": "http://archives.ismir.net/ismir2019/paper/000033.pdf",
    "pages": "284-291",
    "abstract": "Guitar tablature is a popular notation guitarists use to learn and share music. As it stands, most tablatures are created by an experienced guitarist taking the time and effort to annotate a song. As the process is time consuming and requires expertise, we are interested in automating this task. Previous approaches to automatic tablature transcription break the problem into two steps: 1) polyphonic pitch estimation, followed by 2) tablature fingering arrangement. Using a convolutional neural network (CNN) model, we can jointly solve both steps by learning a mapping directly from audio data to tablature. The model can simultaneously leverage physical playability constraints and differences in string timbres implicit in the data to determine the actual fingerings being used by the guitarist. We propose TabCNN, a CNN for estimating guitar tablature from audio of a solo acoustic guitar performance. We train and test our network using microphone recordings from the GuitarSet dataset, and TabCNN outperforms a state-of-the-art multipitch estimation algorithm. We also introduce a set of metrics to evaluate guitar tablature estimation.",
    "zenodo_id": 3527800,
    "dblp_key": null,
    "extra": {
      "takeaway": "We propose a guitar tablature estimation system that uses a convolutional neural network to predict fingerings used by the guitarist from audio of an acoustic guitar performance.",
      "external_links": [
        "https://github.com/andywiggins/tab-cnn"
      ],
      "session_position": "B-15"
    }
  },
  {
    "title": "Learning a Joint Embedding Space of Monophonic and Mixed Music Signals for Singing Voice",
    "author": [
      "Kyungyun Lee",
      "Juhan Nam"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527802",
    "url": "https://doi.org/10.5281/zenodo.3527802",
    "ee": "http://archives.ismir.net/ismir2019/paper/000034.pdf",
    "pages": "295-302",
    "abstract": "Previous approaches in singer identification have used one of monophonic vocal tracks or mixed tracks containing multiple instruments, leaving a semantic gap between these two domains of audio. In this paper, we present a system to learn a joint embedding space of monophonic and mixed tracks for singing voice. We use a metric learning method, which ensures that tracks from both domains of the same singer are mapped closer to each other than those of different singers. We train the system on a large synthetic dataset generated by music mashup to reflect real-world music recordings. Our approach opens up new possibilities for cross-domain tasks, e.g., given a monophonic track of a singer as a query, retrieving mixed tracks sung by the same singer from the database. Also, it requires no additional vocal enhancement steps such as source separation. We show the effectiveness of our system for singer identification and singer-based music retrieval in both the in-domain and cross-domain tasks.",
    "zenodo_id": 3527802,
    "dblp_key": null,
    "extra": {
      "takeaway": "The paper introduces a new method of obtaining a consistent singing voice representation from both monophonic and mixed music signals. Also, it presents a simple music mashup pipeline to create a large synthetic singer dataset. ",
      "external_links": [
        "http://github.com/kyungyunlee/mono2mixed-singer"
      ],
      "session_position": "C-01"
    }
  },
  {
    "title": "Augmenting Music Listening Experiences on Voice Assistants",
    "author": [
      "Morteza Behrooz",
      "Sarah Mennicken",
      "Jennifer Thom",
      "Rohit Kumar",
      "Henriette Cramer"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527804",
    "url": "https://doi.org/10.5281/zenodo.3527804",
    "ee": "http://archives.ismir.net/ismir2019/paper/000035.pdf",
    "pages": "303-310",
    "abstract": "Voice interfaces have rapidly gained popularity, introducing the opportunity for new ways to explore new interaction paradigms for music. However, most interactions with music in current consumer voice devices are still relatively transactional; primarily allowing for keyword-based commands and basic content playback controls. They are less likely to contextualize content or support content discovery beyond what users think to ask for. We present an approach to dynamically augment the voice-based music experience with background information using story generation techniques. Our findings indicate that augmentation can have positive effects on voice-based music experiences, given the right user context and mindset.",
    "zenodo_id": 3527804,
    "dblp_key": null,
    "extra": {
      "takeaway": "Using metadata about playlists, artists, and tracks, we present an approach inspired by story generation techniques to dynamically augment music streaming sessions on smart speakers with contextualized transitions.",
      "external_links": [],
      "session_position": "C-02"
    }
  },
  {
    "title": "Coupled Recurrent Models for Polyphonic Music Composition",
    "author": [
      "John Thickstun",
      "Zaid Harchaoui",
      "Dean Foster",
      "Sham Kakade"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527806",
    "url": "https://doi.org/10.5281/zenodo.3527806",
    "ee": "http://archives.ismir.net/ismir2019/paper/000036.pdf",
    "pages": "311-318",
    "abstract": "This paper introduces a novel recurrent model for music composition that is tailored to the structure of polyphonic music. We propose an efficient new conditional probabilistic factorization of musical scores, viewing a score as a collection of concurrent, coupled sequences: i.e. voices. To model the conditional distributions, we borrow ideas from both convolutional and recurrent neural models; we argue that these ideas are natural for capturing music's pitch invariances, temporal structure, and polyphony. We train models for single-voice and multi-voice composition on 2,300 scores from the KernScores dataset.",
    "zenodo_id": 3527806,
    "dblp_key": null,
    "extra": {
      "takeaway": "This paper investigates automatic music composition via parameterized, probabilistic models of scores. We consider ways to exploit the structure of music to strengthen these models, borrowing ideas from convolutional and recurrent neural networks.",
      "external_links": [
        "http://homes.cs.washington.edu/~thickstn/ismir2019composition/"
      ],
      "session_position": "C-03"
    }
  },
  {
    "title": "Hit Song Prediction: Leveraging Low- and High-Level Audio Features",
    "author": [
      "Eva Zangerle",
      "Michael V\u00f6tter",
      "Ramona Huber",
      "Yi-Hsuan Yang"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527808",
    "url": "https://doi.org/10.5281/zenodo.3527808",
    "ee": "http://archives.ismir.net/ismir2019/paper/000037.pdf",
    "pages": "319-326",
    "abstract": "Assessing the potential success of a given song based on its acoustic characteristics is an important task in the music industry. This task has mostly been approached from an internal perspective, utilizing audio descriptors to predict the success of a given song, where either low- or high-level audio features have been utilized separately. In this work, we aim to jointly exploit low- and high-level audio features and model the prediction as a regression task. Particularly, we make use of a wide and deep neural network architecture that allows for jointly exploiting low- and high-level features. Furthermore, we enrich the set of features with information about the release year of tracks. We evaluate our approach based on the Million Song Dataset and characterize a song as a hit if it is contained in the Billboard Hot 100 at any point in time. Our findings suggest that the proposed approach is able to outperform baseline approaches as well as approaches utilizing low- or high-level features individually. Furthermore, we find that incorporating the release year as well as features describing mood and vocals of a song improve prediction results.",
    "zenodo_id": 3527808,
    "dblp_key": null,
    "extra": {
      "takeaway": "We show that for predicting the potential success of a song,  both low- and high-level audio features are important. We use a deep and wide neural network to model these features and perform a regression task on the track\u2019s rank in the charts.",
      "external_links": [
        "https://doi.org/10.5281/zenodo.3258042"
      ],
      "session_position": "C-04"
    }
  },
  {
    "title": "Da-TACOS: A Dataset for Cover Song Identification and Understanding",
    "author": [
      "Furkan Yesiler",
      "Chris Tralie",
      "Albin Correya",
      "Diego Furtado Silva",
      "Philip Tovstogan",
      "Emilia Gomez",
      "Xavier Serra"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527810",
    "url": "https://doi.org/10.5281/zenodo.3527810",
    "ee": "http://archives.ismir.net/ismir2019/paper/000038.pdf",
    "pages": "327-334",
    "abstract": "This paper focuses on Cover Song Identification (CSI), an important research challenge in content-based Music Information Retrieval (MIR). Although the task itself is interesting and challenging for both academia and industry scenarios, there are a number of limitations for the advancement of current approaches. We specifically address two of them in the present study. First, the number of publicly available datasets for this task is limited, and there is no publicly available benchmark set that is widely used among researchers for comparative algorithm evaluation. Second, most of the algorithms are not publicly shared and reproducible, limiting the comparison of approaches. To overcome these limitations we propose Da-TACOS, a DaTAset for COver Song Identification and Understanding, and two frameworks for feature extraction and benchmarking to facilitate reproducibility. Da-TACOS contains 25K songs represented by unique editorial metadata plus 9 low- and mid-level features pre-computed with open source libraries, and is divided into two subsets. The Cover Analysis subset contains audio features (e.g. key, tempo) that can serve to study how musical characteristics vary for cover songs. The Benchmark subset contains the set of features that have been frequently used in CSI research, e.g. chroma, MFCC, beat onsets etc. Moreover, we provide initial benchmarking results of a selected number of state-of-the-art CSI algorithms using our dataset, and for reproducibility, we share a GitHub repository containing the feature extraction and benchmarking frameworks.",
    "zenodo_id": 3527810,
    "dblp_key": null,
    "extra": {
      "takeaway": "This work aims to understand the links among cover songs with computational approaches and to improve reproducibility of Cover Song Identification task by providing a benchmark dataset and frameworks for comparative algorithm evaluation.",
      "external_links": [],
      "session_position": "C-05"
    }
  },
  {
    "title": "Harmonic Syntax in Time: Rhythm Improves Grammatical Models of Harmony",
    "author": [
      "Daniel Harasim",
      "Timothy O'Donnell",
      "Martin Rohrmeier"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527812",
    "url": "https://doi.org/10.5281/zenodo.3527812",
    "ee": "http://archives.ismir.net/ismir2019/paper/000039.pdf",
    "pages": "335-342",
    "abstract": "Music is hierarchically structured, both in how it is perceived by listeners and how it is composed. Such structure can be captured elegantly using probabilistic grammatical models similar to those used to study natural language. They address the complexity of the structure using abstract categories in a recursive formalism. Most existing grammatical models of musical structure focus on one single dimension of music--such as melody, harmony, or rhythm. While these grammar models often work well on short musical excerpts, accurate analysis of longer pieces requires taking into account the constraints from multiple domains of structure. The present paper proposes abstract product grammars--a formalism which integrates multiple dimensions of musical structure into a single grammatical model--along with efficient parsing and inference algorithms for this formalism. We use this model to study the combination of hierarchically-structured harmonic syntax and hierarchically-structured rhythmic information. The latter is modeled by a novel grammar of rhythm that is capable of expressing temporal regularities in musical phrases. It integrates grouping structure and meter. The combined model of harmony and rhythm outperforms both single-dimension models in computational experiments. All models are trained and evaluated on a treebank of hand-annotated Jazz standards.",
    "zenodo_id": 3527812,
    "dblp_key": null,
    "extra": {
      "takeaway": "This paper integrates rhythm into harmonic syntax models of harmony using a novel grammar of rhythmic phrases.",
      "external_links": [],
      "session_position": "C-06"
    }
  },
  {
    "title": "Learning to Traverse Latent Spaces for Musical Score Inpainting",
    "author": [
      "Ashis Pati",
      "Alexander Lerch",
      "Ga\u00ebtan Hadjeres"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527814",
    "url": "https://doi.org/10.5281/zenodo.3527814",
    "ee": "http://archives.ismir.net/ismir2019/paper/000040.pdf",
    "pages": "343-351",
    "abstract": "Music Inpainting is the task of filling in missing or lost information in a piece of music. We investigate this task from an interactive music creation perspective. To this end, a novel deep learning-based approach for musical score inpainting is proposed. The designed model takes both past and future musical context into account and is capable of suggesting ways to connect them in a musically meaningful manner. To achieve this, we leverage the representational power of the latent space of a Variational Auto-Encoder and train a Recurrent Neural Network which learns to traverse this latent space conditioned on the past and future musical contexts. Consequently, the designed model is capable of generating several measures of music to connect two musical excerpts. The capabilities and performance of the model are showcased by comparison with competitive baselines using several objective and subjective evaluation methods. The results show that the model generates meaningful inpaintings and can be used in interactive music creation applications. Overall, the method demonstrates the merit of learning complex trajectories in the latent spaces of deep generative models.",
    "zenodo_id": 3527814,
    "dblp_key": null,
    "extra": {
      "takeaway": "Recurrent Neural Networks can be trained using latent embeddings of a Variational Auto-Encoder-based model to  to perform interactive music generation tasks such as inpainting. ",
      "external_links": [
        "https://ashispati.github.io/inpaintnet/",
        "https://github.com/ashispati/InpaintNet"
      ],
      "session_position": "C-07"
    }
  },
  {
    "title": "Detecting Stable Regions in Frequency Trajectories for Tonal Analysis of Traditional Georgian Vocal Music",
    "author": [
      "Sebastian Rosenzweig",
      "Frank Scherbaum",
      "Meinard M\u00fcller"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527816",
    "url": "https://doi.org/10.5281/zenodo.3527816",
    "ee": "http://archives.ismir.net/ismir2019/paper/000041.pdf",
    "pages": "352-359",
    "abstract": "While Georgia has a long history of orally transmitted polyphonic singing, there is still an ongoing controversial discussion among ethnomusicologists on the tuning system underlying this type of music. First attempts have been made to analyze tonal properties (e. g., harmonic and melodic intervals) based on fundamental frequency (F0) trajectories. One major challenge in F0-based tonal analysis is introduced by unstable regions in the trajectories due to pitch slides and other frequency fluctuations. In this paper, we describe two approaches for detecting stable regions in frequency trajectories: the first algorithm uses morphological operations inspired by image processing, and the second one is based on suitably defined binary time\u2013frequency masks. To avoid undesired distortions in subsequent analysis steps, both approaches keep the original F0-values unmodified, while only removing F0-values in unstable trajectory regions. We evaluate both approaches against manually annotated stable regions and discuss their potential in the context of interval analysis for traditional three-part Georgian singing.",
    "zenodo_id": 3527816,
    "dblp_key": null,
    "extra": {
      "takeaway": "This paper gives a mathematically rigorous description of two conceptually different approaches (one based on morphological operations, the other based on binary time-frequency masks) for detecting stable regions in frequency trajectories.",
      "external_links": [
        "https://www.audiolabs-erlangen.de/resources/MIR/2019-ISMIR-StableF0"
      ],
      "session_position": "C-08"
    }
  },
  {
    "title": "The AcousticBrainz Genre Dataset: Multi-Source, Multi-Level, Multi-Label, and Large-Scale",
    "author": [
      "Dmitry Bogdanov",
      "Alastair Porter",
      "Hendrik Schreiber",
      "Juli\u00e1n Urbano",
      "Sergio Oramas"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527818",
    "url": "https://doi.org/10.5281/zenodo.3527818",
    "ee": "http://archives.ismir.net/ismir2019/paper/000042.pdf",
    "pages": "360-367",
    "abstract": "This paper introduces the AcousticBrainz Genre Dataset, a large-scale collection of hierarchical multi-label genre annotations from different metadata sources. It allows researchers to explore how the same music pieces are annotated differently by different communities following their own genre taxonomies, and how this could be addressed by genre recognition systems. Genre labels for the dataset are sourced from both expert annotations and crowds, permitting comparisons between strict hierarchies and folksonomies. Music features are available via the AcousticBrainz database. To guide research, we suggest a concrete research task and provide a baseline as well as an evaluation method. This task may serve as an example of the development and validation of automatic annotation algorithms on complementary datasets with different taxonomies and coverage. With this dataset, we hope to contribute to developments in content-based music genre recognition as well as cross-disciplinary studies on genre metadata analysis.",
    "zenodo_id": 3527818,
    "dblp_key": null,
    "extra": {
      "takeaway": "The AcousticBrainz Genre Dataset allows researchers to explore how the same music pieces are annotated differently by different communities following their own genre taxonomies, and how these differences can be addressed by genre recognition systems.",
      "external_links": [
        "https://mtg.github.io/acousticbrainz-genre-dataset/"
      ],
      "session_position": "C-09"
    }
  },
  {
    "title": "Data-Driven Song Recognition Estimation Using Collective Memory Dynamics Models",
    "author": [
      "Christos Koutlis",
      "Manos Schinas",
      "Vasiliki Gkatziaki",
      "Symeon Papadopoulos",
      "Yiannis  Kompatsiaris"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527820",
    "url": "https://doi.org/10.5281/zenodo.3527820",
    "ee": "http://archives.ismir.net/ismir2019/paper/000043.pdf",
    "pages": "368-375",
    "abstract": "Cultural products such as music tracks intend to be appreciated and recognized by a portion of the audience. However, no matter how highly recognized a song might be at the beginning of its life, its recognition will inevitably and progressively decay. The mechanism that governs this decreasing trajectory could be modelled as a forgetting curve or a collective memory decay process. Here, we propose a composite model, termed T-REC, that involves chart data, YouTube views, Spotify popularity of tracks and forgetting curve dynamics with the purpose of estimating song recognition levels. We also present a comparative study, involving state-of-the-art and baseline models based on ground truth data from a survey that we conducted regarding the recognition level of 100 songs in Sweden. Our method is found to perform best among this ensemble of models. A remarkable finding of our study pertains to the role of the number of weeks a song remains in the charts, which is found to be a major factor for the accurate estimation of the song recognition level.",
    "zenodo_id": 3527820,
    "dblp_key": null,
    "extra": {
      "takeaway": "In this paper a composite track recognition model based on chart data, YouTube views and Spotify popularity is proposed and is evaluated on real data obtained from a survey conducted in Sweden.",
      "external_links": [
        "https://zenodo.org/record/3255311#.XRMTcogzaUk",
        "https://zenodo.org/record/3257096#.XRMhiYgzaUk"
      ],
      "session_position": "C-10"
    }
  },
  {
    "title": "Towards Interpretable Polyphonic Transcription with Invertible Neural Networks",
    "author": [
      "Rainer Kelz",
      "Gerhard Widmer"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527822",
    "url": "https://doi.org/10.5281/zenodo.3527822",
    "ee": "http://archives.ismir.net/ismir2019/paper/000044.pdf",
    "pages": "376-383",
    "abstract": "We explore a novel way of conceptualising the task of polyphonic music transcription, using so-called invertible neural networks. Invertible models unify both discriminative and generative aspects in one function, sharing one set of parameters. Introducing invertibility enables the practitioner to directly inspect what the discriminative model has learned, and exactly determine which inputs lead to which outputs. For the task of transcribing polyphonic audio into symbolic form, these models may be especially useful as they allow us to observe, for instance, to what extent the concept of single notes could be learned from a corpus of polyphonic music alone (which has been identified as a serious problem in recent research). This is an entirely new approach to audio transcription, which first of all necessitates some groundwork. In this paper, we begin by looking at the simplest possible invertible transcription model, and then thoroughly investigate its properties. Finally, we will take first steps towards a more sophisticated and capable version. We use the task of piano transcription, and specifically the MAPS dataset, as a basis for these investigations. ",
    "zenodo_id": 3527822,
    "dblp_key": null,
    "extra": {
      "takeaway": "Invertible Neural Networks enable direct interpretability of the latent space.",
      "external_links": [
        "https://github.com/rainerkelz/ISMIR19"
      ],
      "session_position": "C-11"
    }
  },
  {
    "title": "Learning to Generate Music With Sentiment",
    "author": [
      "Lucas Ferreira",
      "Jim Whitehead"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527824",
    "url": "https://doi.org/10.5281/zenodo.3527824",
    "ee": "http://archives.ismir.net/ismir2019/paper/000045.pdf",
    "pages": "384-390",
    "abstract": "Deep Learning models have shown very promising results in automatically composing polyphonic music pieces. However, it is very hard to control such models in order to guide the compositions towards a desired goal. We are interested in controlling a model to automatically generate music with a given sentiment. This paper presents a generative Deep Learning model that can be directed to compose music with a given sentiment. Besides music generation, the same model can be used for sentiment analysis of symbolic music. We evaluate the accuracy of the model in classifying sentiment of symbolic music using a new dataset of video game soundtracks. Results show that our model is able to obtain good prediction accuracy.  A user study shows that human subjects agreed that the generated music has the intended sentiment, however negative pieces can be ambiguous.",
    "zenodo_id": 3527824,
    "dblp_key": null,
    "extra": {
      "takeaway": "A new LSTM method for generating symbolic music with sentiment.",
      "external_links": [],
      "session_position": "C-12"
    }
  },
  {
    "title": "Backtracking Search Heuristics for Solving the All-partition Array Problem",
    "author": [
      "Brian Bemman",
      "David Meredith"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527826",
    "url": "https://doi.org/10.5281/zenodo.3527826",
    "ee": "http://archives.ismir.net/ismir2019/paper/000046.pdf",
    "pages": "391-397",
    "abstract": "Recent efforts to model the compositional processes of Milton Babbitt have yielded a number of computationally challenging problems. One of these problems, known as the \\textit{all-partition array problem}, is a particularly hard variant of set covering, and several different approaches, including mathematical optimization, constraint satisfaction, and greedy backtracking, have been proposed for solving it. Of these previous approaches, only constraint programming has led to a successful solution. Unfortunately, this solution is expensive in terms of computation time. We present here two new search heuristics and a modification to a previously proposed heuristic, that, when applied to a greedy backtracking algorithm, allow the all-partition array problem to be solved in a practical running time. We demonstrate the success of our heuristics by solving for three different instances of the problem found in Babbitt's music, including one previously solved with constraint programming and one Babbitt himself was unable to solve. Use of the new heuristics allows each instance of the problem to be solved more quickly than was possible with previous approaches.",
    "zenodo_id": 3527826,
    "dblp_key": null,
    "extra": {
      "takeaway": "This paper provides search heuristics for use with a greedy backtracking algorithm which solve a hard variant of a set-covering problem found in 12-tone serial music.",
      "external_links": [],
      "session_position": "C-13"
    }
  },
  {
    "title": "Modeling and Learning Structural Breaks in Sonata Forms",
    "author": [
      "Laurent Feisthauer",
      "Louis Bigo",
      "Mathieu Giraud"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527828",
    "url": "https://doi.org/10.5281/zenodo.3527828",
    "ee": "http://archives.ismir.net/ismir2019/paper/000047.pdf",
    "pages": "398-404",
    "abstract": "Expositions of Sonata Forms are structured towards two cadential goals, one being the Medial Caesura (MC). The MC is a gap in the musical texture between the Transition zone (TR) and the Secondary thematic zone (S). It appears as a climax of energy accumulation initiated by the TR, dividing the Exposition in two parts.   We introduce high-level features relevant to formalize this energy gain and to identify MCs. These features concern rhythmic, harmonic and textural aspects of the music and characterize either the MC, its preparation or the texture contrast between TR and S. They are used to train a LSTM neural network on a corpus of 27 movements of string quartets written by Mozart. The model correctly locates the MCs on 14 movements within a leave-one-piece-out validation strategy. We discuss these results and how the network manages to model such structural breaks. ",
    "zenodo_id": 3527828,
    "dblp_key": null,
    "extra": {
      "takeaway": "We trained a neural network with high-level musical feature to find medial caesura in string quartet movements written by Mozart. It finds correctly the MC for a little over half of the corpus.  ",
      "external_links": [
        "http://www.algomus.fr/data"
      ],
      "session_position": "C-14"
    }
  },
  {
    "title": "Auto-adaptive Resonance Equalization using Dilated Residual Networks",
    "author": [
      "Maarten Grachten",
      "Emmanuel Deruty",
      "Alexandre Tanguy"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527830",
    "url": "https://doi.org/10.5281/zenodo.3527830",
    "ee": "http://archives.ismir.net/ismir2019/paper/000048.pdf",
    "pages": "405-411",
    "abstract": "In music and audio production, attenuation of spectral resonances is an important step towards a technically correct result. In this paper we present a two-component system to automate the task of resonance equalization. The first component is a dynamic equalizer that automatically detects resonances and offers to attenuate them by a user-specified factor. The second component is a deep neural network that predicts the optimal attenuation factor based on the windowed audio. The network is trained and validated on empirical data gathered from an experiment in which sound engineers choose their preferred attenuation factors for a set of tracks. We test two distinct network architectures for the predictive model and find that a dilated residual network operating directly on the audio signal is on a par with a network architecture that requires a prior audio feature extraction stage. Both architectures predict human-preferred resonance attenuation factors significantly better than a baseline approach.",
    "zenodo_id": 3527830,
    "dblp_key": null,
    "extra": {
      "takeaway": "We propose a method to fully automate resonance equalization in mixing and mastering musical audio. The method predicts the resonance attenuation factor using neural networks trained and evaluated on ground truth collected from sound engineers. ",
      "external_links": [],
      "session_position": "C-15"
    }
  },
  {
    "title": "Analyzing User Interactions with Music Information Retrieval System: An Eye-tracking Approach",
    "author": [
      "Xiao Hu",
      "Ying Que",
      "Noriko Kando",
      "Wenwei Lian"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527832",
    "url": "https://doi.org/10.5281/zenodo.3527832",
    "ee": "http://archives.ismir.net/ismir2019/paper/000049.pdf",
    "pages": "415-422",
    "abstract": "There has been little research considering eye movement as a measure when assessing user interactions with music information retrieval (MIR) systems, whereas many studies have adopted conventional user-centered measures such as user effectiveness and user perception. To bridge this research gap, this study investigates users' eye movement patterns and measures with two music retrieval tasks and two interface presentation modes. A user experiment was conducted with 16 participants whose eye movement and mouse click behaviors were recorded through professional eye trackers. Through analyzing visual patterns of eye gazes and movements as well as various metrics in prominent Areas of Interest (AOI), it is found that users\u2019 eye movement behaviors were related to task type. Besides, the results also disclosed that some eye movement metrics were related to both user effective-ness and user perception, and influenced by user characteristics. It is also found that some eye movement and user effectiveness metrics can be used to predict user perception. This study allows researchers to gain a deeper insight into user interactions with MIR systems from the perspective of eye movement measure.",
    "zenodo_id": 3527832,
    "dblp_key": null,
    "extra": {
      "takeaway": "Eye movement measures can be used in investigating user interactions with MIR systems.",
      "external_links": [],
      "session_position": "D-01"
    }
  },
  {
    "title": "A Cross-Scape Plot Representation for Visualizing Symbolic Melodic Similarity",
    "author": [
      "Saebyul Park",
      "Taegyun Kwon",
      "Jongpil Lee",
      "Jeounghoon Kim",
      "Juhan Nam"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527834",
    "url": "https://doi.org/10.5281/zenodo.3527834",
    "ee": "http://archives.ismir.net/ismir2019/paper/000050.pdf",
    "pages": "423-430",
    "abstract": "Symbolic melodic similarity is based on measuring a pairwise distance between two songs from diverse perspectives. The distance is usually summarized as a single value for song retrieval. This obscures observing the details of similarity patterns within the two songs. In this paper, we propose a cross-scape plot representation to visualize multi-scaled melody similarity between two symbolic music. The cross-scape plot is computed by stacking up a minimum local distance between two segments from each of the two songs. As the layer goes up, the segment size increases and it computes incrementally more long-term distances. This hierarchical representation allows for capturing the location and length of similar segments between two songs in a visually intuitive manner. We show the effectiveness of the cross-scape plot by evaluating it on examples from folk music collections with similarity-based categories and plagiarism cases. ",
    "zenodo_id": 3527834,
    "dblp_key": null,
    "extra": {
      "takeaway": "We propose a cross-scape plot representation to visualize multi-scaled melody similarity between two symbolic music. We evaluate its effectiveness on examples from folk music collections with similarity-based categories and plagiarism cases.",
      "external_links": [
        "https://github.com/saebyulpark/cross_scapeplot_visualization"
      ],
      "session_position": "D-02"
    }
  },
  {
    "title": "JosquIntab: A Dataset for Content-based Computational Analysis of Music in Lute Tablature",
    "author": [
      "Reinier de Valk",
      "Ryaan Ahmed",
      "Tim Crawford"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527836",
    "url": "https://doi.org/10.5281/zenodo.3527836",
    "ee": "http://archives.ismir.net/ismir2019/paper/000051.pdf",
    "pages": "431-438",
    "abstract": "An enormous corpus of music for the lute, spanning some two and half centuries, survives today. Unlike other musical corpora from the same period, this corpus has undergone only limited musicological study. The main reason for this is that it is written down exclusively in lute tablature, a prescriptive form of notation that is difficult to understand for non-specialists as it reveals little structural information. In this paper we present JosquIntab, a dataset of automatically created enriched diplomatic transcriptions in MIDI and MEI format of 64 sixteenth-century lute intabulations, instrumental arrangements of vocal compositions. Such a dataset enables large-scale content-based computational analysis of music in lute tablature hitherto impossible. We describe the dataset, the mapping algorithm used to create it, as well as a method to quantitatively evaluate the degree of arrangement (goodness of fit) of an intabulation. Furthermore, we present two use cases, demonstrating the usefulness of the dataset for both music information retrieval and musicological research. We make the dataset, the source code, and an implementation of the mapping algorithm, runnable as a command line tool, publicly available.",
    "zenodo_id": 3527836,
    "dblp_key": null,
    "extra": {
      "takeaway": "We present JosquIntab, a dataset of automatically created transcriptions (MIDI/MEI) of 64 lute intabulations; the creation algorithm; and our evaluation method. In two use cases, we demonstrate its usefulness for both MIR and musicological research.",
      "external_links": [
        "https://github.com/reinierdevalk/data/",
        "https://github.com/reinierdevalk/tabmapper/"
      ],
      "session_position": "D-03"
    }
  },
  {
    "title": "A Dataset of Rhythmic Pattern Reproductions and Baseline Automatic Assessment System",
    "author": [
      "Felipe Falc\u00e3o",
      "Bar\u0131\u015f Bozkurt",
      "Xavier Serra",
      "Nazareno Andrade",
      "Ozan Baysal"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527838",
    "url": "https://doi.org/10.5281/zenodo.3527838",
    "ee": "http://archives.ismir.net/ismir2019/paper/000052.pdf",
    "pages": "439-445",
    "abstract": "This work presents a novel dataset comprised of audio and jury evaluations for rhythmic pattern reproduction performances by students applying for a conservatory. Data was collected in-loco during entrance exams where students were asked to imitated a set of rhythmic patterns played by teachers. In addition to the pass or fail grades provided by the members of the jury during the exam sessions, a subset of the data was also evaluated by external annotators on a 4-level scale. A baseline automatic assessment system is presented to demonstrate the usefulness of the dataset. Preliminary results deliver an accuracy of 76% for a simple pass/fail logistic regression classifier and a mean average error of 0.59 for a linear regression grade estimator. The implementation is also made publicly available to serve as baseline for alternative assessments systems that may leverage the dataset.",
    "zenodo_id": 3527838,
    "dblp_key": null,
    "extra": {
      "takeaway": "This present work is an effort to address the shortage of music datasets designed for rhythmic assessment. A new dataset and baseline rhythmic assessment system are provided in order to support comparative studies about rhythmic assessment.",
      "external_links": [
        "https://zenodo.org/record/2620357",
        "https://zenodo.org/record/2619499",
        "https://github.com/MTG/mast-rhythm-analysis"
      ],
      "session_position": "D-04"
    }
  },
  {
    "title": "Self-Supervised Methods for Learning Semantic Similarity in Music",
    "author": [
      "Mason Bretan",
      "Larry Heck"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527840",
    "url": "https://doi.org/10.5281/zenodo.3527840",
    "ee": "http://archives.ismir.net/ismir2019/paper/000053.pdf",
    "pages": "446-453",
    "abstract": "Neural networks have been used to learn a latent \"musical space\" or \"embedding\" to encode meaningful features and provide a method of measuring semantic similarity between two musical passages. An ideal embedding is one that both captures features useful for downstream tasks and conforms to a distribution suitable for sampling and meaningful interpolation. We present two new methods for learning musical embeddings that leverage context while simultaneously imposing a shape on the feature space distribution via backpropagation using an adversarial component. We focus on the symbolic domain and target short polyphonic musical units consisting of 40 note sequences. The goal is to project these units into a continuous low dimensional space that has semantic relevance. We evaluate relevance based on the learned features' abilities to complete various musical tasks and show improvement over baseline models including variational autoencoders, adversarial autoencoders, and deep structured semantic models. We use a dataset consisting of classical piano and demonstrate the robustness of our methods across multiple input representations.",
    "zenodo_id": 3527840,
    "dblp_key": null,
    "extra": {
      "takeaway": "By combining self-supervised learning techniques based on contextual prediction with adversarial training we demonstrate it is possible to impose a prior distribution on a learned latent space without degrading the quality of the features.",
      "external_links": [],
      "session_position": "D-05"
    }
  },
  {
    "title": "Blending Acoustic and Language Model Predictions for Automatic Music Transcription",
    "author": [
      "Adrien Ycart",
      "Andrew McLeod",
      "Emmanouil Benetos",
      "Kazuyoshi Yoshii"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527842",
    "url": "https://doi.org/10.5281/zenodo.3527842",
    "ee": "http://archives.ismir.net/ismir2019/paper/000054.pdf",
    "pages": "454-461",
    "abstract": "In this paper, we introduce a method for converting an input probabilistic piano roll (the output of a typical multi-pitch detection model) into a binary piano roll. The task is an important step for many automatic music transcription systems with the goal of converting an audio recording into some symbolic format. Our model has two components: an LSTM-based music language model (MLM) which can be trained on any MIDI data, not just that aligned with audio; and a blending model used to combine the probabilities of the MLM with those of the input probabilistic piano roll given by an acoustic multi-pitch detection model, which must be trained on (a comparably small amount of) aligned data. We use scheduled sampling to make the MLM robust to noisy sequences during testing. We analyze the performance of our model on the MAPS dataset using two different timesteps (40ms and 16th-note), comparing it against a strong baseline hidden Markov model with a training method not used before for the task to our knowledge. We report a statistically significant improvement over HMM decoding in terms of notewise F-measure with both timesteps, with 16th note timesteps improving further compared to 40ms timesteps.",
    "zenodo_id": 3527842,
    "dblp_key": null,
    "extra": {
      "takeaway": "Dynamically integrating predictions from an acoustic and a language model with a blending model improves automatic music transcription performance on the MAPS dataset. Results are further improved by operating on 16th-note timesteps rather than 40ms.",
      "external_links": [
        "http://c4dm.eecs.qmul.ac.uk/ycart/ismir19.html"
      ],
      "session_position": "D-06"
    }
  },
  {
    "title": "Modelling the Syntax of North Indian Melodies with a Generalized Graph Grammar",
    "author": [
      "Christoph Finkensiep",
      "Richard Widdess",
      "Martin Rohrmeier"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527844",
    "url": "https://doi.org/10.5281/zenodo.3527844",
    "ee": "http://archives.ismir.net/ismir2019/paper/000055.pdf",
    "pages": "462-469",
    "abstract": "Hierarchical models of music allow explanation of highly complex musical structure based on the general principle of recursive elaboration and a small set of orthogonal operations. Recent approaches to melodic elaboration have converged to a representation based on intervals, which allows the elaboration of pairs of notes. However, two problems remain: First, an interval-first representation obscures one-sided operations like neighbor notes. Second, while models of Western melody styles largely agree on step-wise operations such as neighbors and passing notes, larger intervals are either attributed to latent harmonic properties or left unexplained. This paper presents a grammar for melodies in North Indian raga music, showing not only that recursively applied neighbor and passing note operations underlie this style as well, but that larger intervals are generated as generalized neighbors, based on the tonal hierarchy of the underlying scale structure. The notion of a generalized neighbor is not restricted to ragas but can be transferred to other musical styles, opening new perspectives on latent structure behind melodies and music in general. The presented grammar is based on a graph representation that allows one to express elaborations on both notes and intervals, unifying and generalizing previous graph- and tree-based approaches.",
    "zenodo_id": 3527844,
    "dblp_key": null,
    "extra": {
      "takeaway": "Note- and interval-based models of hierarchical structure can be unified with a graph representation. Furthermore, leaps in melodies can be explained by latent structures such as the relative stability of pitches in a mode.",
      "external_links": [
        "https://zenodo.org/record/3258808"
      ],
      "session_position": "D-07"
    }
  },
  {
    "title": "A Comparative Study of Neural Models for Polyphonic Music Sequence Transduction",
    "author": [
      "Adrien Ycart",
      "Daniel Stoller",
      "Emmanouil Benetos"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527846",
    "url": "https://doi.org/10.5281/zenodo.3527846",
    "ee": "http://archives.ismir.net/ismir2019/paper/000056.pdf",
    "pages": "470-477",
    "abstract": "Automatic transcription of polyphonic music remains a challenging task in the field of Music Information Retrieval. One under-investigated point is the post-processing of time-pitch posteriograms into binary piano rolls. In this study, we investigate this task using a variety of neural network models and training procedures.  We introduce an adversarial framework, that we compare against more traditional training losses. We also propose the use of binary neuron outputs and compare them to the usual real-valued outputs in both training frameworks. This allows us to train networks directly using the F-measure as training objective. We evaluate these methods using two kinds of transduction networks and two different multi-pitch detection systems, and compare the results against baseline note-tracking methods on a dataset of classical piano music. Analysis of results indicates that (1) convolutional models improve results over baseline models, but no improvement is reported for recurrent models; (2) supervised losses are superior to adversarial ones; (3) binary neurons do not improve results; (4) cross-entropy loss results in better or equal performance compared to the F-measure loss.",
    "zenodo_id": 3527846,
    "dblp_key": null,
    "extra": {
      "takeaway": "A systematic study using various neural models and automatic music transcription systems shows that a cross-entropy-loss CNN improves transduction performance, while an LSTM does not. Using an adversarial set-up also does not yield improvement.",
      "external_links": [],
      "session_position": "D-08"
    }
  },
  {
    "title": "Learning Similarity Metrics for Melody Retrieval",
    "author": [
      "Folgert Karsdorp",
      "Peter Kranenburg",
      "Enrique Manjavacas"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527848",
    "url": "https://doi.org/10.5281/zenodo.3527848",
    "ee": "http://archives.ismir.net/ismir2019/paper/000057.pdf",
    "pages": "478-485",
    "abstract": "Similarity measures are indispensable in music information retrieval. In recent years, various proposals have been made for measuring melodic similarity in symbolically encoded scores. Many of these approaches are ultimately based on a dynamic programming approach such as sequence alignment or edit distance, which has various drawbacks. First, the similarity scores are not necessarily metrics and are not directly comparable. Second, the algorithms are mostly first-order and of quadratic time-complexity, and finally, the features and weights need to be defined precisely. We propose an alternative approach which employs deep neural networks for end-to-end similarity metric learning. We contrast and compare different recurrent neural architectures (LSTM and GRU) for representing symbolic melodies as continuous vectors, and demonstrate how duplet and triplet loss functions can be employed to learn compact distributional representations of symbolic music in an induced melody space. This approach is contrasted with an alignment-based approach. We present results for the Meertens Tune Collections, which consists of a large number of vocal and instrumental monophonic pieces from Dutch musical sources, spanning five centuries, and demonstrate the robustness of the learned similarity metrics.",
    "zenodo_id": 3527848,
    "dblp_key": null,
    "extra": {
      "takeaway": "We compare different recurrent neural architectures to represent symbolic melodies as continuous vectors. We show how duplet and triplet loss functions can be used to learn distributional representations of symbolic music in an induced melody space.",
      "external_links": [
        "https://github.com/fbkarsdorp/melodic-similarity/blob/master/Karsdorp_ISMIR2019_SI.pdf"
      ],
      "session_position": "D-09"
    }
  },
  {
    "title": "Multi-Task Learning of Tempo and Beat: Learning One to Improve the Other",
    "author": [
      "Sebastian B\u00f6ck",
      "Matthew Davies",
      "Peter Knees"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527850",
    "url": "https://doi.org/10.5281/zenodo.3527850",
    "ee": "http://archives.ismir.net/ismir2019/paper/000058.pdf",
    "pages": "486-493",
    "abstract": "In this paper, we propose a multi-task learning approach for simultaneous tempo estimation and beat tracking of musical audio. The system shows state-of-the-art performance for both tasks on a wide range of data, but has another fundamental advantage: due to its multi-task nature, it is not only able to exploit the mutual information of both tasks by learning a common, shared representation, but can also improve one by learning only from the other. The multi-task learning is achieved by globally aggregating the skip connections of a beat tracking system built around temporal convolutional networks, and feeding them into a tempo classification layer. The benefit of this approach is investigated by the inclusion of training data for which tempo-only annotations are available, and which is shown to provide improvements in beat tracking accuracy.",
    "zenodo_id": 3527850,
    "dblp_key": null,
    "extra": {
      "takeaway": "Multi-task learning helps to improve beat tracking accuracy if additional tempo information is used.",
      "external_links": [
        "https://github.com/superbock/ISMIR2019"
      ],
      "session_position": "D-10"
    }
  },
  {
    "title": "Can We Increase Inter- and Intra-Rater Agreement in Modeling General Music Similarity?",
    "author": [
      "Arthur Flexer",
      "Taric Lallai"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527852",
    "url": "https://doi.org/10.5281/zenodo.3527852",
    "ee": "http://archives.ismir.net/ismir2019/paper/000059.pdf",
    "pages": "494-500",
    "abstract": "We present a pilot study on ways to increase inter- and intra-rater agreement in quantification of general similarity between pieces of music. By using a more controlled group of human subjects and carefully curating song material, we try to increase overall agreement between raters concerning the perceived general similarity of songs. Repeated conduction of the experiment with a two week lag shows that intra-rater agreement is higher than inter-rater agreement. Analysis of the results and interviews with test subjects suggests that the genre of songs was a major factor in judging similarity between songs. We discuss the impacts of our results on evaluation of respective machine learning models and question the validity of experiments on general music similarity.",
    "zenodo_id": 3527852,
    "dblp_key": null,
    "extra": {
      "takeaway": "Models of general music similarity are problematic due to the subjective nature of music perception, which is shown and discussed by conducting a user experiment trying to improve the MIREX `Audio Music Similarity' task.",
      "external_links": [
        "http://ofai.at/~arthur.flexer/song_list_ismir2019.txt"
      ],
      "session_position": "D-11"
    }
  },
  {
    "title": "AIST Dance Video Database: Multi-Genre, Multi-Dancer, and Multi-Camera Database for Dance Information Processing",
    "author": [
      "Shuhei Tsuchida",
      "Satoru Fukayama",
      "Masahiro Hamasaki",
      "Masataka Goto"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527854",
    "url": "https://doi.org/10.5281/zenodo.3527854",
    "ee": "http://archives.ismir.net/ismir2019/paper/000060.pdf",
    "pages": "501-510",
    "abstract": "We describe the AIST Dance Video Database (AIST Dance DB), a shared database containing original street dance videos with copyright-cleared dance music. Although dancing is highly related to dance music and dance information can be considered an important aspect of music information, research on dance information processing has not yet received much attention in the Music Information Retrieval (MIR) community. We therefore developed the AIST Dance DB as the first large-scale shared database focusing on street dances to facilitate research on a variety of tasks related to dancing to music. It consists of 13,939 dance videos covering 10 major dance genres as well as 60 pieces of dance music composed for those genres. The videos were recorded by having 40 professional dancers (25 male and 15 female) dance to those pieces. We carefully designed this database so that it can cover both solo dancing and group dancing as well as both basic choreography moves and advanced moves originally choreographed by each dancer. Moreover, we used multiple cameras surrounding a dancer to simultaneously shoot from various directions. The AIST Dance DB will foster new MIR tasks such as dance-motion genre classification, dancer identification, and dance-technique estimation. We propose a dance-motion genre-classification task and developed four baseline methods of identifying dance genres of videos in this database. We evaluated these methods by extracting dancer body motions and training their classifiers on the basis of long short-term memory (LSTM) recurrent neural network models and support-vector machine (SVM) models.",
    "zenodo_id": 3527854,
    "dblp_key": null,
    "extra": {
      "takeaway": "AIST Dance Video Database is the first large-scale database containing original street dance videos with copyright-cleared music. It accelerates research of dance information processing such as dance-motion classification and dancer identification.",
      "external_links": [
        "https://aistdancedb.ongaaccel.jp"
      ],
      "session_position": "D-12"
    }
  },
  {
    "title": "Microtiming Analysis in Traditional Shetland Fiddle Music",
    "author": [
      "Estefania Cano",
      "Scott Beveridge"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527856",
    "url": "https://doi.org/10.5281/zenodo.3527856",
    "ee": "http://archives.ismir.net/ismir2019/paper/000061.pdf",
    "pages": "511-516",
    "abstract": "This work aims to characterize microtiming variations in traditional Shetland fiddle music.  These microtiming variations dictate the rhythmic flow of a performed melody, and contribute, among other things, to the suitability of this music as an accompaniment to dancing. In the context of Shetland fiddle music, these microtiming variations are often referred to as lilt.  Using a corpus of 27 traditional fiddle tunes from the Shetland Isles, we examine inter-beat timing deviations, as well as inter-onset timing deviations of eighth note sequences.  Results show a number of distinct inter-beat and inter-onset rhythmic patterns that may characterize lilt, as well as idiosyncratic patterns for each performer.  This paper presents a first step towards the use of Music Information Retrieval (MIR) techniques for modelling lilt in traditional Scottish fiddle music, and highlights its implications in the field of ethnomusicology.",
    "zenodo_id": 3527856,
    "dblp_key": null,
    "extra": {
      "takeaway": "The analysis of microtiming variations on a corpus of Shetland fiddle music, revealed characteristic patterns in the duration of beats and eighth notes that may be related to the suitability of fiddle music as an accompaniment to dancing.",
      "external_links": [
        "https://github.com/ecanoc/ShetlandFiddles"
      ],
      "session_position": "D-13"
    }
  },
  {
    "title": "SUPRA: Digitizing the Stanford University Piano Roll Archive",
    "author": [
      "Zhengshan Shi",
      "Craig Sapp",
      "Kumaran Arul",
      "Jerry  McBride",
      "Julius Smith"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527858",
    "url": "https://doi.org/10.5281/zenodo.3527858",
    "ee": "http://archives.ismir.net/ismir2019/paper/000062.pdf",
    "pages": "517-523",
    "abstract": "This paper describes the digitization process of a large collection of historical piano roll recordings held in the Stanford University Piano Roll Archive (SUPRA), which has resulted in an initial dataset of 478 performances of pianists from the early twentieth century transcribed to MIDI format. The process includes scanning paper rolls, digitizing the hole punches, and translating the pneumatic expression codings into MIDI format to create expressive performance files. We offer derivative files from each step of this process, including a high resolution image of the roll, a \u201craw\u201d MIDI file of hole data, an \u201cexpressive\u201d MIDI file that translates hole data into dynamics, and an audio file rendering of the expressive MIDI file on a digital piano sample. This provides digital access to the rolls for researchers in a flexible, searchable online database. We currently offer an initial dataset, \u201cSUPRA-RW\u201d from a selection of \u201cred Welte\u201d-type rolls in the SUPRA. This dataset provides roll scans and MIDI transcriptions of important historical piano performances, many being made available widely for the first time.",
    "zenodo_id": 3527858,
    "dblp_key": null,
    "extra": {
      "takeaway": "This paper describes the digitization process of SUPRA, an online database of historical piano roll recordings, which has resulted in an initial dataset of 478 performances of pianists from the early twentieth century transcribed to MIDI format.",
      "external_links": [
        "https://supra.stanford.edu/"
      ],
      "session_position": "D-14"
    }
  },
  {
    "title": "Fast and Flexible Neural Audio Synthesis",
    "author": [
      "Lamtharn Hantrakul",
      "Jesse Engel",
      "Adam Roberts",
      "Chenjie Gu",
      "Lamtharn Hantrakul"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527860",
    "url": "https://doi.org/10.5281/zenodo.3527860",
    "ee": "http://archives.ismir.net/ismir2019/paper/000063.pdf",
    "pages": "524-530",
    "abstract": "Autoregressive neural networks, such as WaveNet, have opened up new avenues for expressive audio synthesis. High-quality speech synthesis utilizes detailed linguistic features for conditioning, but comparable levels of control have yet to be realized for neural synthesis of musical instruments. Here, we demonstrate an autoregressive model capable of synthesizing realistic audio that closely follows fine-scale temporal conditioning for loudness and fundamental frequency. We find the appropriate choice of conditioning features and architectures improves both the quantitative accuracy of audio resynthesis and qualitative responsiveness to creative manipulation of conditioning. While large autoregressive models generate audio much slower than real-time, we achieve these results with a more efficient WaveRNN model, opening the door for exploring real-time interactive audio synthesis with neural networks.",
    "zenodo_id": 3527860,
    "dblp_key": null,
    "extra": {
      "takeaway": "We present an autoregressive WaveRNN model capable of synthesizing realistic audio that closely follows fine-scale temporal conditioning for loudness and fundamental frequency.",
      "external_links": [
        "https://storage.googleapis.com/ffnsynth/supp/index.html"
      ],
      "session_position": "D-15"
    }
  },
  {
    "title": "DeepSRGM - Sequence Classification and Ranking in Indian Classical Music Via Deep Learning",
    "author": [
      "Sathwik Tejaswi Madhusudhan",
      "Girish Chowdhary"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527862",
    "url": "https://doi.org/10.5281/zenodo.3527862",
    "ee": "http://archives.ismir.net/ismir2019/paper/000064.pdf",
    "pages": "533-540",
    "abstract": "A vital aspect of Indian Classical Music (ICM) is Raga, which serves as a melodic framework for compositions and improvisations alike. Raga Recognition is an important music information retrieval task in ICM as it can aid numerous downstream applications ranging from music recommendations to organizing huge music collections. In this work, we propose a deep learning based approach to Raga recognition. Our approach employs efficient pre-possessing and learns temporal sequences in music data using Long Short Term Memory based Recurrent Neural Networks (LSTM-RNN). We train and test the network on smaller sequences sampled from the original audio while the final inference is performed on the audio as a whole. Our method achieves an accuracy of 88.1% and 97 % during inference on theComp Music Carnatic dataset and its 10 Raga subset respectively making it the state-of-the-art for the Raga recognition task. Our approach also enables sequence ranking which aids us in retrieving melodic patterns from a given music database that are closely related to the presented query sequence.",
    "zenodo_id": 3527862,
    "dblp_key": null,
    "extra": {
      "takeaway": "In this work, we propose deep learning based methods for Raga recognition and sequence ranking in Indian classical music. Our approach employs efficient pre-possessing and learns temporal sequences in music data using LSTM Recurrent Neural Networks.",
      "external_links": [],
      "session_position": "E-01"
    }
  },
  {
    "title": "Modeling Music Modality with a Key-Class Invariant Pitch Chroma CNN",
    "author": [
      "Anders Elowsson",
      "Anders Friberg"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527864",
    "url": "https://doi.org/10.5281/zenodo.3527864",
    "ee": "http://archives.ismir.net/ismir2019/paper/000065.pdf",
    "pages": "541-548",
    "abstract": "This paper presents a convolutional neural network (CNN) that uses input from a polyphonic pitch estimation system to predict perceived minor/major modality in music audio. The pitch activation input is structured to allow the first CNN layer to compute two pitch chromas focused on different octaves. The following layers perform harmony analysis across chroma and time scales. Through max pooling across pitch, the CNN becomes invariant with regards to the key class (i.e., key disregarding mode) of the music. A multilayer perceptron combines the modality activation output with spectral features for the final prediction. The study uses a dataset of 203 excerpts rated by around 20 listeners each, a small challenging data size requiring a carefully designed parameter sharing. With an R2 of about 0.71, the system clearly outperforms previous systems as well as individual human listeners. A final ablation study highlights the importance of using pitch activations processed across longer time scales, and using pooling to facilitate invariance with regards to the key class.",
    "zenodo_id": 3527864,
    "dblp_key": null,
    "extra": {
      "takeaway": "When analyzing musical harmony with a CNN it can be beneficial to: start from a pretrained pitch transcription system using deep layered learning, compute a pitch chroma within the CNN, and promote key invariance through pooling across key class.",
      "external_links": [],
      "session_position": "E-02"
    }
  },
  {
    "title": "Convolutional Composer Classification",
    "author": [
      "Harsh Verma",
      "John Thickstun"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527866",
    "url": "https://doi.org/10.5281/zenodo.3527866",
    "ee": "http://archives.ismir.net/ismir2019/paper/000066.pdf",
    "pages": "549-556",
    "abstract": "This paper investigates end-to-end learnable models for attributing composers to musical scores. We introduce several pooled, convolutional architectures for this task and draw connections between our approach and classical learning approaches based on global and n-gram features. We evaluate models on a corpus of 2,500 scores from the KernScores collection, authored by a variety of composers spanning the Renaissance era to the early 20th century. This corpus has substantial overlap with the corpora used in several previous, smaller studies; we compare our results on subsets of the corpus to these previous works.",
    "zenodo_id": 3527866,
    "dblp_key": null,
    "extra": {
      "takeaway": "This paper investigates the effectiveness simple convolutional models for attributing composers to musical scores, evaluated on a corpus of 2,500 scores authored by a variety of composers spanning the Renaissance era to the early 20th century.",
      "external_links": [
        "https://homes.cs.washington.edu/~thickstn/ismir2019classification/"
      ],
      "session_position": "E-03"
    }
  },
  {
    "title": "A Diplomatic Edition of Il Lauro Secco: Ground Truth for OMR of White Mensural Notation",
    "author": [
      "Emilia Parada-Cabaleiro",
      "Anton Batliner",
      "Bj\u00f6rn Schuller"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527868",
    "url": "https://doi.org/10.5281/zenodo.3527868",
    "ee": "http://archives.ismir.net/ismir2019/paper/000067.pdf",
    "pages": "557-564",
    "abstract": "Early musical sources in white mensural notation\u2014the most common notation in European printed music during the Renaissance\u2014are nowadays preserved by libraries worldwide trough digitalisation. Still, the application of music information retrieval to this repertoire is restricted by the use of digitalisation techniques which produce an uncodified output. Optical Music Recognition (OMR) automatically generates a symbolic representation of image-based musical content, thus making this repertoire reachable from the computational point of view; yet, further improvements are often constricted by the limited ground truth available. We address this lacuna by presenting a symbolic representation in original notation of Il Lauro Secco, an anthology of Italian madrigals in white mensural notation. For musicological analytic purposes, we encoded the repertoire in **mens and MEI formats; for OMR ground truth, we automatically codified the repertoire in agnostic and semantic formats, via conversion from the **mens files.",
    "zenodo_id": 3527868,
    "dblp_key": null,
    "extra": {
      "takeaway": "We present a symbolic representation in mensural notation of the anthology Il Lauro Secco. For musicological analysis we encoded the repertoire in **mens and MEI; to support OMR research we present ground truth in agnostic and semantic formats.",
      "external_links": [
        "https://github.com/SEILSdataset/SEILSdataset"
      ],
      "session_position": "E-04"
    }
  },
  {
    "title": "The Harmonix Set: Beats, Downbeats, and Functional Segment Annotations of Western Popular Music",
    "author": [
      "Oriol Nieto",
      "Matthew McCallum",
      "Matthew Davies",
      "Andrew Robertson",
      "Adam Stark",
      "Eran Egozy"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527870",
    "url": "https://doi.org/10.5281/zenodo.3527870",
    "ee": "http://archives.ismir.net/ismir2019/paper/000068.pdf",
    "pages": "565-572",
    "abstract": "We introduce the Harmonix set: a collection of annotations of beats, downbeats, and functional segmentation for over 900 full tracks that covers a wide range of western popular music. Given the variety of annotated music information types in this set, and how strongly these three types of data are typically intertwined, we seek to foster research that focuses on multiple retrieval tasks at once. The dataset in- cludes additional metadata such as MusicBrainz identifiers to support the linking of the dataset to third-party informa- tion or audio data when available. We describe the method- ology employed in acquiring this set, including the annota- tion process and song selection. In addition, an initial data exploration of the annotations and actual dataset content is conducted. Finally, we provide a series of baselines of the Harmonix set with reference beat-trackers, downbeat estimation, and structural segmentation algorithms.",
    "zenodo_id": 3527870,
    "dblp_key": null,
    "extra": {
      "takeaway": "Human annotated dataset containing beats, downbeats, and structural segmentation for over 900 pop tracks.",
      "external_links": [
        "https://github.com/urinieto/harmonixset"
      ],
      "session_position": "E-05"
    }
  },
  {
    "title": "FMP Notebooks: Educational Material for Teaching and Learning Fundamentals of Music Processing",
    "author": [
      "Meinard M\u00fcller",
      "Frank Zalkow"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527872",
    "url": "https://doi.org/10.5281/zenodo.3527872",
    "ee": "http://archives.ismir.net/ismir2019/paper/000069.pdf",
    "pages": "573-580",
    "abstract": "In this paper, we introduce a novel collection of educational material for teaching and learning fundamentals of music processing (FMP) with a particular focus on the audio domain. This collection, referred to as FMP notebooks, discusses well-established topics in Music Information Retrieval (MIR) as motivating application scenarios. The FMP notebooks provide detailed textbook-like explanations of central techniques and algorithms in combination with Python code examples that illustrate how to implement the theory. All components including the introductions of MIR scenarios, illustrations, sound examples, technical concepts, mathematical details, and code examples are integrated into a consistent and comprehensive framework based on Jupyter notebooks. The FMP notebooks are suited for studying the theory and practice, for generating educational material for lectures, as well as for providing baseline implementations for many MIR tasks, thus addressing students, teachers, and researchers. ",
    "zenodo_id": 3527872,
    "dblp_key": null,
    "extra": {
      "takeaway": "The FMP notebooks include open-source Python code, Jupyter notebooks, detailed explanations, as well as numerous audio and music examples for teaching and learning MIR and audio signal processing.",
      "external_links": [
        "https://www.audiolabs-erlangen.de/FMP"
      ],
      "session_position": "E-06"
    }
  },
  {
    "title": "Automatic Assessment of Sight-reading Exercises",
    "author": [
      "Jiawen Huang",
      "Alexander Lerch"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527876",
    "url": "https://doi.org/10.5281/zenodo.3527876",
    "ee": "http://archives.ismir.net/ismir2019/paper/000070.pdf",
    "pages": "581-587",
    "abstract": "Sight-reading requires a musician to decode, process, and perform a musical score quasi-instantaneously and without rehearsal. Due to the complexity of this task, it is difficult to assess the proficiency of a sight-reading performance, and it is even more challenging to model its human assessment. This study aims at evaluating and identifying effective features for automatic assessment of sight-reading performance. The evaluated set of features comprises task-specific, hand-crafted, and interpretable features designed to represent various aspect of sight-reading performance covering parameters such as intonation, timing, dynamics, and score continuity. The most relevant features are identified by Principal Component Analysis and forward feature selection. For context, the same features are also applied to the assessment of rehearsed student music performances and compared across different assessment categories. The results show potential of automatic assessment models for sight-reading and the relevancy of different features as well as the contribution of different feature groups to different assessment categories.",
    "zenodo_id": 3527876,
    "dblp_key": null,
    "extra": {
      "takeaway": "This paper shows the relevancy of different features as well as the contribution of different feature groups to different assessment categories for sight-reading exercises.",
      "external_links": [],
      "session_position": "E-07"
    }
  },
  {
    "title": "Supervised Symbolic Music Style Translation Using Synthetic Data",
    "author": [
      "Ond\u0159ej C\u00edfka",
      "Umut Simsekli",
      "Ga\u00ebl Richard"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527878",
    "url": "https://doi.org/10.5281/zenodo.3527878",
    "ee": "http://archives.ismir.net/ismir2019/paper/000071.pdf",
    "pages": "588-595",
    "abstract": "Research on style transfer and domain translation has clearly demonstrated the ability of deep learning-based algorithms to manipulate images in terms of artistic style. More recently, several attempts have been made to extend such approaches to music (both symbolic and audio) in order to enable transforming musical style in a similar manner. In this study, we focus on symbolic music with the goal of altering the \u2018style\u2019 of a piece while keeping its original \u2018content\u2019. As opposed to the current methods, which are inherently restricted to be unsupervised due to the lack of \u2018aligned\u2019 data (i.e. the same musical piece played in multiple styles), we develop the first fully supervised algorithm for this task. At the core of our approach lies a synthetic data generation scheme which allows us to produce virtually unlimited amounts of aligned data, and hence avoid the above issue. In view of this data generation scheme, we propose an encoder-decoder model for translating symbolic music accompaniments between a number of different styles. Our experiments show that our models, although trained entirely on synthetic data, are capable of producing musically meaningful accompaniments even for real (non-synthetic) MIDI recordings.",
    "zenodo_id": 3527878,
    "dblp_key": null,
    "extra": {
      "takeaway": "Synthetic data is useful for learning to efficiently transform musical style.",
      "external_links": [
        "https://doi.org/10.5281/zenodo.3250606",
        "https://git.io/musicstyle"
      ],
      "session_position": "E-08"
    }
  },
  {
    "title": "Deep Music Analogy Via Latent Representation Disentanglement",
    "author": [
      "Ruihan Yang",
      "Dingsu Wang",
      "Ziyu Wang",
      "Tianyao Chen",
      "Junyan Jiang",
      "Gus Xia"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527880",
    "url": "https://doi.org/10.5281/zenodo.3527880",
    "ee": "http://archives.ismir.net/ismir2019/paper/000072.pdf",
    "pages": "596-603",
    "abstract": "Analogy-making is a key method for computer algorithms to generate both natural and creative music pieces. In general, an analogy is made by partially transferring the music abstractions, i.e., high-level representations and their relationships, from one piece to another; however, this procedure requires disentangling music representations, which usually takes little effort for musicians but is non-trivial for computers. Three sub-problems arise: extracting latent representations from the observation, disentangling the representations so that each part has a unique semantic interpretation, and mapping the latent representations back to actual music. In this paper, we contribute an explicitly-constrained variational autoencoder (EC^2-VAE) as a unified solution to all three sub-problems. We focus on disentangling the pitch and rhythm representations of 8-beat music clips conditioned on chords. In producing music analogies, this model helps us to realize the imaginary situation of \"what if\" a piece is composed using a different pitch contour, rhythm pattern, or chord progression by borrowing the representations from other pieces. Finally, we validate the proposed disentanglement method using objective measurements and evaluate the analogy examples by a subjective study.",
    "zenodo_id": 3527880,
    "dblp_key": null,
    "extra": {
      "takeaway": "We contribute a representation disentanglement method tailored for music composition, which enables to achieve domain-free music analogy-making.",
      "external_links": [
        "https://github.com/cdyrhjohn/Deep-Music-Analogy-Demos"
      ],
      "session_position": "E-09"
    }
  },
  {
    "title": "Query by Video: Cross-modal Music Retrieval",
    "author": [
      "Bochen Li",
      "Aparna Kumar"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527882",
    "url": "https://doi.org/10.5281/zenodo.3527882",
    "ee": "http://archives.ismir.net/ismir2019/paper/000073.pdf",
    "pages": "604-611",
    "abstract": "Cross-modal retrieval learns the relationship between the two types of data in a common space so that an input from one modality can retrieve data from a different modality. We focus on modeling the relationship between two highly diverse data, music and real-world videos. We learn cross-modal embeddings using a two-stream network trained with music-video pairs. Each branch takes one modality as the input and it is constrained with emotion tags. Then the constraints allow the cross-modal embeddings to be learned with significantly fewer music-video pairs. To retrieve music for an input video, the trained model ranks tracks in the music database by cross-modal distances to the query video. Quantitative evaluations show high accuracy of audio/video emotion tagging when evaluated on each branch independently and high performance for cross-modal music retrieval. We also present cross-modal music retrieval experiments on Spotify music using user-generated videos from Instagram and Youtube as queries, and subjective evaluations show that the proposed model can retrieve relevant music. We present the music retrieval results at: http://www.ece.rochester.edu/~bli23/projects/query.html.",
    "zenodo_id": 3527882,
    "dblp_key": null,
    "extra": {
      "takeaway": "This paper presents a cross-modal distance learning model to retrieve music for videos based on emotion concepts. The emotion constraints on the model allow for efficient training.",
      "external_links": [
        "http://www.ece.rochester.edu/~bli23/projects/query.html"
      ],
      "session_position": "E-10"
    }
  },
  {
    "title": "Investigating CNN-based Instrument Family Recognition for Western Classical Music Recordings",
    "author": [
      "Michael Taenzer",
      "Jakob Abe\u00dfer",
      "Stylianos I. Mimilakis",
      "Christof Weiss",
      "Meinard M\u00fcller"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527884",
    "url": "https://doi.org/10.5281/zenodo.3527884",
    "ee": "http://archives.ismir.net/ismir2019/paper/000074.pdf",
    "pages": "612-619",
    "abstract": "Western classical music comprises a rich repertoire composed for different ensembles. Often, these ensembles consist of instruments from one or two of the families woodwinds, brass, piano, vocals, and strings. In this paper, we consider the task of automatically recognizing instrument families from music recordings. As one main contribution, we investigate the influence of data normalization, pre-processing, and augmentation techniques on the generalization capability of the models. We report on experiments using three datasets of monotimbral recordings covering different levels of timbral complexity: isolated notes, isolated melodies, and polyphonic pieces. While data augmentation and the normalization of spectral patches turned out to be beneficial, pre-processing strategies such as logarithmic compression and channel-energy normalization did not lead to substantial improvements. Furthermore, our cross-dataset experiments indicate the necessity of further optimization routines such as domain adaptation.",
    "zenodo_id": 3527884,
    "dblp_key": null,
    "extra": {
      "takeaway": "This paper describes extensive experiments for CNN-based instrument family recognition systems. In particular, it studies the effect of data normalization, pre-processing, and augmentation techniques on the generalization capability of the models.",
      "external_links": [
        "https://doi.org/10.5281/zenodo.3258829"
      ],
      "session_position": "E-11"
    }
  },
  {
    "title": "A Bi-Directional Transformer for Musical Chord Recognition",
    "author": [
      "Jonggwon Park",
      "Kyoyun Choi",
      "Sungwook Jeon",
      "Dokyun Kim",
      "Jonghun Park"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527886",
    "url": "https://doi.org/10.5281/zenodo.3527886",
    "ee": "http://archives.ismir.net/ismir2019/paper/000075.pdf",
    "pages": "620-627",
    "abstract": "Chord recognition is an important task since chords are highly abstract and descriptive features of music. For effective chord recognition, it is essential to utilize relevant context in audio sequence. While various machine learning models such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have been employed for the task, most of them have limitations in capturing long-term dependency or require training of an additional model.  In this work, we utilize a self-attention mechanism for chord recognition to focus on certain regions of chords. Training of the proposed bi-directional Transformer for chord recognition (BTC) consists of a single phase while showing competitive performance. Through an attention map analysis, we have visualized how attention was performed. It turns out that the model was able to divide segments of chords by utilizing adaptive receptive field of the attention mechanism. Furthermore, it was observed that the model was able to effectively capture long-term dependencies, making use of essential information regardless of distance.",
    "zenodo_id": 3527886,
    "dblp_key": null,
    "extra": {
      "takeaway": "We propose bi-directional Transformer model based on self-attention mechanism for chord recognition. Through an attention map analysis, we visualize how attention was performed and conclude that the model can effectively capture long-term dependency.",
      "external_links": [
        "https://github.com/jayg996/BTC-ISMIR19"
      ],
      "session_position": "E-12"
    }
  },
  {
    "title": "SAMBASET: A Dataset of Historical Samba de Enredo Recordings for Computational Music Analysis",
    "author": [
      "Lucas Maia",
      "Magdalena Fuentes",
      "Luiz Biscainho",
      "Mart\u00edn Rocamora",
      "Slim Essid"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527888",
    "url": "https://doi.org/10.5281/zenodo.3527888",
    "ee": "http://archives.ismir.net/ismir2019/paper/000076.pdf",
    "pages": "628-635",
    "abstract": "In the last few years, several datasets have been released to meet the requirements of \"hungry\" yet promising data-driven approaches in modern music technology research. Since, for historical reasons, most investigations conducted in the field still revolve around music of the so-called \"Western\" tradition, the corresponding data, methodology and conclusions carry a strong cultural bias. Music of non-\"Western\" background, whenever present, is usually underrepresented, poorly labeled, or even mislabeled, the exception being projects that aim at specifically describing such music. In this paper we present SAMBASET, a dataset of Brazilian samba music that contains over 40 hours of historical and modern samba de enredo commercial recordings. To the best of our knowledge, this is the first dataset of this genre. We describe the collection of metadata (e.g. artist, composer, release date) and outline our semiautomatic approach to the challenging task of annotating beats in this large dataset, which includes the assessment of the performance of state-of-the-art beat tracking algorithms for this specific case. Finally, we present a study on tempo and beat tracking that illustrates SAMBASET's value, and we comment on other tasks for which it could be used.",
    "zenodo_id": 3527888,
    "dblp_key": null,
    "extra": {
      "takeaway": "SAMBASET is a large samba de enredo dataset that includes rich metadata, beat and downbeat annotations. It could provide challenges to state-of-the-art algorithms in MIR tasks such as rhythmic analysis, vocal F0 and chord estimation, among others.",
      "external_links": [
        "http://www02.smt.ufrj.br/~starel/sambaset/"
      ],
      "session_position": "E-13"
    }
  },
  {
    "title": "Deep-Rhythm for Global Tempo Estimation in Music",
    "author": [
      "Hadrien Foroughmand",
      "Geoffroy Peeters"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527890",
    "url": "https://doi.org/10.5281/zenodo.3527890",
    "ee": "http://archives.ismir.net/ismir2019/paper/000077.pdf",
    "pages": "636-643",
    "abstract": "It has been shown that the harmonic series at the tempo frequency of the onset-strength-function of an audio signal accurately describes its rhythm pattern and can be used to perform tempo or rhythm pattern estimation. Recently, in the case of multi-pitch estimation, the depth of the input layer of a convolutional network has been used to represent the harmonic series of pitch candidates. We use a similar idea here to represent the harmonic series of tempo candidates. We propose the Harmonic-Constant-Q-Modulation which represents, using a 4D-tensors, the harmonic series of  modulation frequencies (considered as tempo frequencies) in several acoustic frequency bands over time. This representation is used as input to a convolutional network which is trained to estimate tempo or rhythm pattern classes. Using a large number of datasets, we evaluate the performance of our approach and compare it with previous approaches.  We show that it slightly increases Accuracy-1 for tempo estimation but not the average-mean-Recall for rhythm pattern recognition.",
    "zenodo_id": 3527890,
    "dblp_key": null,
    "extra": {
      "takeaway": "Estimation of tempo or rhythm description using a new 4D representation of the harmonic series related to tempo used  as an input to a convolutional neural network which is trained to estimate the tempo or the rhythm pattern classes.",
      "external_links": [],
      "session_position": "E-14"
    }
  },
  {
    "title": "Large-vocabulary Chord Transcription Via Chord Structure Decomposition",
    "author": [
      "Junyan Jiang",
      "Ke Chen",
      "Wei Li",
      "Gus Xia"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527892",
    "url": "https://doi.org/10.5281/zenodo.3527892",
    "ee": "http://archives.ismir.net/ismir2019/paper/000078.pdf",
    "pages": "644-651",
    "abstract": "While audio chord recognition systems have acquired considerable accuracy on small vocabularies (e.g., major/minor chords), the large-vocabulary chord recognition problem still remains unsolved. This problem hinders the practical usages of audio recognition systems. The difficulty mainly lies in the intrinsic long-tail distribution of chord qualities, and most chord qualities have too few samples for model training.  In this paper, we propose a new model for audio chord recognition under a huge chord vocabulary. The core concept is to decompose any chord label into a set of musically meaningful components (e.g., triad, bass, seventh), each with a much smaller vocabulary compared to the size of the overall chord vocabulary. A multitask classifier is then trained to recognize all the components given the audio feature, and then labels of individual components are reassembled to form the final chord label. Experiments show that the proposed system not only achieves state-of-the-art results on traditional evaluation metrics but also performs well on a large vocabulary. ",
    "zenodo_id": 3527892,
    "dblp_key": null,
    "extra": {
      "takeaway": "In this paper, we propose a new model for large-vocabulary chord recognition by chord structure decomposition with state-of-the-art performance on different metrics.",
      "external_links": [],
      "session_position": "E-15"
    }
  },
  {
    "title": "BandNet: A Neural Network-based, Multi-Instrument Beatles-Style MIDI Music Composition Machine",
    "author": [
      "Yichao Zhou",
      "Wei Chu",
      "Sam Young",
      "Xin Chen"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527894",
    "url": "https://doi.org/10.5281/zenodo.3527894",
    "ee": "http://archives.ismir.net/ismir2019/paper/000079.pdf",
    "pages": "655-662",
    "abstract": "In this paper, we propose a recurrent neural network (RNN)-based MIDI music composition machine that is able to learn musical knowledge from existing Beatles' music and generate full songs in the style of the Beatles with little human intervention. In the learning stage, a sequence of stylistically uniform, multiple-channel music samples was modeled by an RNN. In the composition stage, a short clip of randomly-generated music was used as a seed for the RNN to start music score prediction. To form structured music, segments of generated music from different seeds were concatenated together. To improve the quality and structure of the generated music, we integrated music theory knowledge into the model, such as controlling the spacing of gaps in the vocal melody, normalizing the timing of chord changes, and requiring notes to be related to the song's key (C major, for example). This integration improved the quality of the generated music as verified by a professional composer. We also conducted a subjective listening test that showed our generated music was close to original music by the Beatles in terms of style similarity, professional quality, and interestingness.",
    "zenodo_id": 3527894,
    "dblp_key": null,
    "extra": {
      "takeaway": "We propose a recurrent neural network (RNN)-based MIDI music composition machine that is able to learn musical knowledge from existing Beatles' music and generate full songs in the style of the Beatles with little human intervention.",
      "external_links": [
        "https://goo.gl/uaLXoB."
      ],
      "session_position": "F-01"
    }
  },
  {
    "title": "Can We Listen To It Together?: Factors Influencing Reception of Music Recommendations and Post-Recommendation Behavior ",
    "author": [
      "Jin Ha Lee",
      "Liz Pritchard",
      "Chris Hubbles"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527896",
    "url": "https://doi.org/10.5281/zenodo.3527896",
    "ee": "http://archives.ismir.net/ismir2019/paper/000080.pdf",
    "pages": "663-669",
    "abstract": "Few prior studies on music recommendations investigate the context in which users receive the recommendations, and what impact the recommendation has on the user. In this paper, we aim to better understand the factors that affect people\u2019s decisions as to whether they choose to listen to music recommendations and how the recommendations impact their music-listening behaviors. We conducted an online survey asking about people\u2019s past experiences on giving and receiving music recommendations. We found that in addition to the aesthetic qualities of music and the respondent\u2019s taste, expectations regarding the delivery (e.g., timing, persistence) of the recommendations, familiarity, trust in the recommender\u2019s abilities, and the rationale for suggestions were important factors. We discuss the implications for the design of music recommenders based on the findings, including better rationale for and accessibility of recommended music, improved saving options, and more targeted delivery at specific times. The data also suggests disparities in how people wish to receive music recommendations and what will influence them to listen to recommendations, versus how they would like to offer recommendations to others. In addition, the findings highlight the importance of music recommendations in people\u2019s existing social relationships and their role in building/improving new relationships.",
    "zenodo_id": 3527896,
    "dblp_key": null,
    "extra": {
      "takeaway": "In addition to the aesthetic qualities of music and the respondent\u2019s taste, expectations regarding the delivery, familiarity, trust in the recommender\u2019s abilities, and the rationale for suggestions affected people\u2019s reception of recommendations.",
      "external_links": [
        "https://tinyurl.com/ISMIR2019LeePritchardHubbles"
      ],
      "session_position": "F-02"
    }
  },
  {
    "title": "Adversarial Learning for Improved Onsets and Frames Music Transcription",
    "author": [
      "Jong Wook Kim",
      "Juan Bello"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527898",
    "url": "https://doi.org/10.5281/zenodo.3527898",
    "ee": "http://archives.ismir.net/ismir2019/paper/000081.pdf",
    "pages": "670-677",
    "abstract": "Automatic music transcription is considered to be one of the hardest problems in music information retrieval, yet recent deep learning approaches have achieved substantial improvements in transcription performance. These approaches commonly employ supervised learning models that predict various time-frequency representations, by minimizing element-wise losses such as the cross entropy function. However, applying the loss in this manner assumes conditional independence of each label given the input, and thus cannot accurately express inter-label dependencies. To address this issue, we introduce an adversarial training scheme that operates directly on the time-frequency representations and makes the output distribution closer to the ground-truth. Through adversarial learning, we achieve a consistent improvement in both frame-level and note-level metrics over Onsets and Frames, a state-of-the-art music transcription model. Our results show that adversarial learning can significantly reduce the error rate while increasing the confidence of the model estimations. Our approach is generic and applicable to any transcription model based on multi-label predictions, which are very common in music signal analysis.",
    "zenodo_id": 3527898,
    "dblp_key": null,
    "extra": {
      "takeaway": "Piano roll prediction in music transcription can be improved by appending an additional loss incurred by an adversarial discriminator.",
      "external_links": [],
      "session_position": "F-03"
    }
  },
  {
    "title": "Automatic Music Transcription and Ethnomusicology: a User Study",
    "author": [
      "Andre Holzapfel",
      "Emmanouil Benetos"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527900",
    "url": "https://doi.org/10.5281/zenodo.3527900",
    "ee": "http://archives.ismir.net/ismir2019/paper/000082.pdf",
    "pages": "678-684",
    "abstract": "Converting an acoustic music signal into music notation using a computer program has been at the forefront of music information research for several decades, as a task referred to as automatic music transcription (AMT). However, current AMT research is still constrained to system development followed by quantitative evaluations; it is still unclear whether the performance of AMT methods is considered sufficient to be used in the everyday practice of music scholars. In this paper, we propose and carry out a user study on evaluating the usefulness of automatic music transcription in the context of ethnomusicology. As part of the study, we recruited 16 participants who were asked to transcribe short musical excerpts either from scratch or using the output of an AMT system as a basis. We collect and analyze quantitative measures such as transcription time and effort, and a range of qualitative feedback from study participants, which includes user needs, criticisms of AMT technologies, and links between perceptual and quantitative evaluations on AMT outputs. The results show no quantitative advantage of using AMT, but important indications regarding appropriate user groups and evaluation measures are provided.",
    "zenodo_id": 3527900,
    "dblp_key": null,
    "extra": {
      "takeaway": "After decades of developing Automatic Music Transcription (AMT) systems, this paper conducts a first user study with experienced transcribers to shed light on the potential and drawbacks of incorporating AMT into manual transcription practice.",
      "external_links": [
        "https://bit.ly/2ZKhmnY"
      ],
      "session_position": "F-04"
    }
  },
  {
    "title": "LakhNES: Improving Multi-instrumental Music Generation with Cross-domain Pre-training",
    "author": [
      "Chris Donahue",
      "Huanru Henry Mao",
      "Yiting Ethan Li",
      "Garrison Cottrell",
      "Julian McAuley"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527902",
    "url": "https://doi.org/10.5281/zenodo.3527902",
    "ee": "http://archives.ismir.net/ismir2019/paper/000083.pdf",
    "pages": "685-692",
    "abstract": "We are interested in the task of generating multi-instrumental music scores. The Transformer architecture has recently shown great promise for the task of piano score generation\u2014here we adapt it to the multi-instrumental setting. Transformers are complex, high-dimensional language models which are capable of capturing long-term structure in sequence data, but require large amounts of data to fit. Their success on piano score generation is partially explained by the large volumes of symbolic data readily available for that domain. We leverage the recently-introduced NES-MDB dataset of four-voice scores from an early video game sound synthesis chip (the NES), which we find to be well-suited to training with the Transformer architecture. To further improve the performance of our model, we propose a pre-training technique to leverage the information in a large collection of heterogeneous music. Despite differences between the two corpora, we find that this pre-training procedure improves both quantitative and qualitative performance for our primary task.",
    "zenodo_id": 3527902,
    "dblp_key": null,
    "extra": {
      "takeaway": "We use transfer learning to improve multi-instrumental music generation by first pre-training a Transformer on a large heterogeneous music dataset (Lakh MIDI) and subsequently fine tuning it on a domain of interest (NES-MDB).",
      "external_links": [
        "https://chrisdonahue.com/LakhNES"
      ],
      "session_position": "F-05"
    }
  },
  {
    "title": "Taking Form: A Representation Standard, Conversion Code, and Example Corpora for Recording, Visualizing, and Studying Analyses of Musical Form",
    "author": [
      "Mark Gotham",
      "Matthew Ireland"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527904",
    "url": "https://doi.org/10.5281/zenodo.3527904",
    "ee": "http://archives.ismir.net/ismir2019/paper/000084.pdf",
    "pages": "693-699",
    "abstract": "We report on new specification standards for representing human analyses of musical form which enable musicians to represent their analytical view of a piece either on the score (where an encoded version is available) or on a spreadsheet. Both of these representations are simple, intuitive, and highly human-readable. Further, we provide code for converting between these formats, as well as a nested bracket representation adopted from computational linguistics which, in turn, can be visualised in familiar tree diagrams to provide 'at a glance' introductions to works. Finally, we provide an initial corpus of analyses/annotations in these formats, report on the practicalities of amassing them, and offer tools for automatic comparison of the works in the corpus based on the content and structure of the annotations. We intend for this resource to be useful to computational musicologists, enabling study of form at scale, and also useful pedagogically to all teachers, students, and appreciators of music from whom projects of this kind can be rather disconnected. The code and corpus can be found at https://github.com/MarkGotham/Taking-Form.",
    "zenodo_id": 3527904,
    "dblp_key": null,
    "extra": {
      "takeaway": "We provide new specification standards for representing human analyses of musical form, along with corpora of examples, and code for working with them.",
      "external_links": [
        "https://github.com/MarkGotham/Taking-Form"
      ],
      "session_position": "F-06"
    }
  },
  {
    "title": "Learning Complex Basis Functions for Invariant Representations of Audio",
    "author": [
      "Stefan Lattner",
      "Monika D\u00f6rfler",
      "Andreas Arzt"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527906",
    "url": "https://doi.org/10.5281/zenodo.3527906",
    "ee": "http://archives.ismir.net/ismir2019/paper/000085.pdf",
    "pages": "700-707",
    "abstract": "Learning features from data has shown to be more successful than using hand-crafted features for many machine learning tasks. In music information retrieval (MIR), features learned from windowed spectrograms are highly variant to transformations like transposition or time-shift. Such variances are undesirable when they are irrelevant for the respective MIR task. We propose an architecture called Complex Autoencoder (CAE) which learns features invariant to orthogonal transformations. Mapping signals onto complex basis functions learned by the CAE results in a transformation-invariant \"magnitude space\" and a transformation-variant \"phase space\". The phase space is useful to infer transformations between data pairs. When exploiting the invariance-property of the magnitude space, we achieve state-of-the-art results in audio-to-score alignment and in repeated section discovery for audio. A PyTorch implementation of the CAE, including the repeated section discovery method is available online.",
    "zenodo_id": 3527906,
    "dblp_key": null,
    "extra": {
      "takeaway": "The \"Complex Autoencoder\" learns features invariant to transposition and time-shift of audio in CQT representation. The features are competitive in a repeated section discovery, and in an audio-to-score alignment task.",
      "external_links": [
        "https://github.com/SonyCSLParis/cae-invar"
      ],
      "session_position": "F-07"
    }
  },
  {
    "title": "Folded CQT RCNN For Real-time Recognition of Instrument Playing Techniques",
    "author": [
      "Jean-Fran\u00e7ois Ducher",
      "Philippe  Esling"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527908",
    "url": "https://doi.org/10.5281/zenodo.3527908",
    "ee": "http://archives.ismir.net/ismir2019/paper/000086.pdf",
    "pages": "708-714",
    "abstract": "In the past years, deep learning has produced state-of-the-art performance in timbre and instrument classification. However, only a few models currently deal with the recognition of advanced Instrument Playing Techniques (IPT). None of them have a real-time approach of this problem. Furthermore, most studies rely on a single sound bank for training and testing. Their methodology provides no assurance as to the generalization of their results to other sounds. In this article, we extend state-of-the-art convolutional neural networks to the classification of IPTs. We build the first IPT corpus from independent sound banks, annotate it with the JAMS standard and make it freely available. Our models yield consistently high accuracies on a homogeneous subset of this corpus. However, only a proper taxonomy of IPTs and specifically defined input transforms offer proper resilience when addressing the \u201cminus-1db\u201d methodology, which assesses the ability of the models to generalize. In particular, we introduce a novel Folded Constant Q-Transform adjusted to the requirements of IPT classification. Finally we discuss the use of our classifier in real-time. ",
    "zenodo_id": 3527908,
    "dblp_key": null,
    "extra": {
      "takeaway": "We extend state-of-the-art deep learning models for instrument recognition to the real-time classification of instrument playing techniques. Our models generalize better with a proper taxonomy and an adapted input transform.",
      "external_links": [
        "https://drive.google.com/open?id=1HYqHxxd2ZDkU2TL_1EXa6WNv9lY37hU9",
        "https://drive.google.com/open?id=1GvS6VQ3iJP6e9MBajEPL0VOXva-iuUmS"
      ],
      "session_position": "F-08"
    }
  },
  {
    "title": "humdrumR: a New Take on an Old Approach to Computational Musicology",
    "author": [
      "Nathaniel Condit-Schultz",
      "Claire Arthur"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527910",
    "url": "https://doi.org/10.5281/zenodo.3527910",
    "ee": "http://archives.ismir.net/ismir2019/paper/000087.pdf",
    "pages": "715-722",
    "abstract": "Musicology research is a fundamentally humanistic endeavor. However, despite the productive work of a small niche of humanities-trained computational musicologists, most cutting-edge digital music research is pursued by scholars whose primary training is scientific or computational, not humanistic. This unfortunate situation is prolonged, at least in part, by the daunting barrier that computer coding presents to humanities scholars with no technical training. In this paper, we present humdrumR (\u201chum-drummer\u201d), a software package designed to afford computational musicology research for both advanced and novice computer coders. Humdrum is a powerful and influential existing computational musicology framework, including the humdrum syntax\u2014a flexible text data format with tens of thousands of extant scores available (Kern Scores)\u2014and the Bash-based humdrum toolkit. HumdrumR is a modern replacement for the humdrum toolkit, based in the data-analysis/statistical programming language R. By combining the flexibility and transparency of the humdrum syntax with the powerful data analysis tools and concise syntax of R, humdrumR offers an appealing new approach to would-be computational musicologists. HumdrumR leverages R\u2019s powerful metaprogramming capabilities to create an extremely expressive and composable syntax, allowing novices to achieve usable analyses quickly while avoiding many coding concepts that are commonly challenging for beginners.",
    "zenodo_id": 3527910,
    "dblp_key": null,
    "extra": {
      "takeaway": "Describes a new software toolkit for computational musicology research.",
      "external_links": [],
      "session_position": "F-09"
    }
  },
  {
    "title": "Tunes Together: Perception and Experience of Collaborative Playlists",
    "author": [
      "So Yeon Park",
      "Audrey Laplante",
      "Jin Ha Lee",
      "Blair Kaneshiro"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527912",
    "url": "https://doi.org/10.5281/zenodo.3527912",
    "ee": "http://archives.ismir.net/ismir2019/paper/000088.pdf",
    "pages": "723-730",
    "abstract": "Music is well established as a means of social connection. In the age of streaming platforms, personalized playlists and recommendations are popular topics in music information retrieval. We bring the focus of music enjoyment back to social connection and examine how technologies can enhance interpersonal relationships, specifically through the context of the collaborative playlist (CP). We conducted an exploratory study of CP users and non-users (N=65) and examined speculative and experienced purposes and outcomes of CPs, as well as general perspectives on music and social connectedness. We derived a CP Framework with three purposes - Practical, Cognitive, and Social - and two connotations - Utility and Orientation. Both users and non-users shared similar perspectives on music-related activities and CP user outcomes. Projected and actual CP purposes differed between groups, however, as did perception of music\u2019s role in connectedness in recent years. These results highlight the importance of music-based social interactions for both groups.",
    "zenodo_id": 3527912,
    "dblp_key": null,
    "extra": {
      "takeaway": "Collaborative playlists (CPs) are critical in bringing back social connectedness to music enjoyment. We characterize purposes and connotations of CPs as well as elucidate similarities and differences between users and non-users with the CP Framework.",
      "external_links": [],
      "session_position": "F-10"
    }
  },
  {
    "title": "A Holistic Approach to Polyphonic Music Transcription with Neural Networks",
    "author": [
      "Miguel Roman",
      "Antonio Pertusa",
      "Jorge Calvo-Zaragoza"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527914",
    "url": "https://doi.org/10.5281/zenodo.3527914",
    "ee": "http://archives.ismir.net/ismir2019/paper/000089.pdf",
    "pages": "731-737",
    "abstract": "We present a framework based on neural networks to extract music scores directly from polyphonic audio in an end-to-end fashion. Most previous Automatic Music Transcription (AMT) methods seek a piano-roll representation of the pitches, that can be further transformed into a score by incorporating tempo estimation, beat tracking, key estimation or rhythm quantization. Unlike these methods, our approach generates music notation directly from the input audio in a single stage. For this, we use a Convolutional Recurrent Neural Network (CRNN) with Connectionist Temporal Classification (CTC) loss function which does not require annotated alignments of audio frames with the score rhythmic information. We trained our model using as input Haydn, Mozart, and Beethoven string quartets and Bach chorales synthesized with different tempos and expressive performances. The output is a textual representation of four-voice music scores based on **kern format. Although the proposed approach is evaluated in a simplified scenario, results show that this model can learn to transcribe scores directly from audio signals, opening a promising avenue towards complete AMT.",
    "zenodo_id": 3527914,
    "dblp_key": null,
    "extra": {
      "takeaway": "A neural network architecture is trained in an end-to-end manner to transcribe music scores in humdrum **kern format from polyphonic audio files.",
      "external_links": [
        "https://github.com/mangelroman/audio2score"
      ],
      "session_position": "F-11"
    }
  },
  {
    "title": "Generalized Metrics for Single-f0 Estimation Evaluation",
    "author": [
      "Rachel Bittner",
      "Juan Jose Bosch"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527916",
    "url": "https://doi.org/10.5281/zenodo.3527916",
    "ee": "http://archives.ismir.net/ismir2019/paper/000090.pdf",
    "pages": "738-745",
    "abstract": "Single-f0 estimation methods, including pitch trackers and melody estimators, have historically been evaluated using a set of common metrics which score estimates frame-wise in terms of pitch and voicing accuracy. \"Voicing\" refers to whether or not a pitch is active, and has historically been regarded as a binary value. However, this has limitations because it is often ambiguous whether a pitch is present or absent, making a binary choice difficult for humans and algorithms alike. For example, when a source fades out or reverberates, the exact point where the pitch is no longer present is unclear. Many single-f0 estimation algorithms select a threshold for when a pitch is active or not, and different choices of threshold drastically affect the results of standard metrics. In this paper, we present a refinement on the existing single-f0 metrics, by allowing the estimated voicing to be represented as a continuous likelihood, and introducing a weighting on frame level pitch accuracy, which considers the energy of the source producing the f0 relative to the energy of the rest of the signal. We compare these metrics experimentally with the previous metrics using a number of algorithms and datasets and discuss the fundamental differences. We show that, compared to the previous metrics, our proposed metrics allow threshold-independent algorithm comparisons.",
    "zenodo_id": 3527916,
    "dblp_key": null,
    "extra": {
      "takeaway": "We show a variety of limitations in widely used metrics for measuring the accuracy of single-f0 estimation systems, and propose a generalization which considers non-binary voicing decisions and a weighted scoring of pitch estimations.",
      "external_links": [
        "http://github.com/juanjobosch/continuousf0eval"
      ],
      "session_position": "F-12"
    }
  },
  {
    "title": "Learning Disentangled Representations of Timbre and Pitch for Musical Instrument Sounds Using Gaussian Mixture Variational Autoencoders",
    "author": [
      "Yin-Jyun Luo",
      "Kat Agres",
      "Dorien Herremans"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527918",
    "url": "https://doi.org/10.5281/zenodo.3527918",
    "ee": "http://archives.ismir.net/ismir2019/paper/000091.pdf",
    "pages": "746-753",
    "abstract": "In this paper, we learn disentangled representations of timbre and pitch for musical instrument sounds. We adapt a framework based on variational autoencoders with Gaussian mixture latent distributions. Specifically, we use two separate encoders to learn distinct latent spaces for timbre and pitch, which form Gaussian mixture components representing instrument identity and pitch, respectively. For reconstruction, latent variables of timbre and pitch are sampled from corresponding mixture components, and are concatenated as the input to a decoder. We show the model's efficacy using latent space visualization, and a quantitative analysis indicates the discriminability of these spaces, even with a limited number of instrument labels for training. The model allows for controllable synthesis of selected instrument sounds by sampling from the latent spaces. To evaluate this, we trained instrument and pitch classifiers using original labeled data. These classifiers achieve high F-scores when tested on our synthesized sounds, which verifies the model\u2019s performance of controllable realistic timbre/pitch synthesis. Our model also enables timbre transfer between multiple instruments, with a single encoder-decoder architecture, which is evaluated by measuring the shift in the posterior of instrument classification. Our in-depth evaluation confirms the model's ability to successfully disentangle timbre and pitch.",
    "zenodo_id": 3527918,
    "dblp_key": null,
    "extra": {
      "takeaway": "We disentangle pitch and timbre of musical instrument sounds by learning separate interpretable latent spaces using Gaussian mixture variational autoencoders. The model is verified by controllable sound synthesis and many-to-many timbre transfer.",
      "external_links": [
        "https://ismir19-217.github.io/sup-material/ismir19-217-sup-material.html"
      ],
      "session_position": "F-13"
    }
  },
  {
    "title": "The ISMIR Explorer - A Visual Interface for Exploring 20 Years of ISMIR Publications",
    "author": [
      "Thomas Low",
      "Christian Hentschel",
      "Sayantan Polley",
      "Anustup Das",
      "Harald Sack",
      "Andreas Nurnberger",
      "Sebastian Stober"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527920",
    "url": "https://doi.org/10.5281/zenodo.3527920",
    "ee": "http://archives.ismir.net/ismir2019/paper/000092.pdf",
    "pages": "754-760",
    "abstract": "Ever since the first International Symposium on Music Information Retrieval in 2000, the proceedings have been made publicly available to interested researchers. After 20 years of annual conferences and workshops, this number has grown to an impressive amount of almost 2,000 papers. When restricted to linear search and retrieval in a document collection of this size, it becomes inherently hard to identify topics, related work and trends in scientific research. Therefore, this paper presents and evaluates a map-based user interface for exploring 20 years of ISMIR publications. The interface visualizes k-nearest neighbor subsets of semantically similar papers. Users may jump from one neighborhood to the next by selecting another paper from the current subset. Through animated transitions between local k-nn maps, the interface creates the impression of panning a large global map. Evaluations results of a small user study suggest that users are able to discover interesting links between papers. Due to its generic approach, the interface is easily applicable to other document collections as well. The search interface and its source code will be made publicly available.",
    "zenodo_id": 3527920,
    "dblp_key": null,
    "extra": {
      "takeaway": "We present a visual user interface for exploring the cumulative ISMIR proceedings based on locally aligned neighborhood maps containing semantically similar papers. Use this to search for related work or to discover interesing new topics!",
      "external_links": [
        "https://ismir-explorer.ai.ovgu.de"
      ],
      "session_position": "F-14"
    }
  },
  {
    "title": "Pattern Clustering in Monophonic Music by Learning a Non-Linear Embedding From Human Annotations",
    "author": [
      "Timothy de Reuse",
      "Ichiro Fujinaga"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527922",
    "url": "https://doi.org/10.5281/zenodo.3527922",
    "ee": "http://archives.ismir.net/ismir2019/paper/000093.pdf",
    "pages": "761-768",
    "abstract": "Musical pattern discovery algorithms find instances of repetition in symbolic music, allowing for some user-specifiable amount of variation between identified repetitions; however, they can yield an intractably large number of discovered patterns when allowing for even small amounts of variation. This is commonly addressed by defining some heuristic notion of pattern significance, and returning only the most significant patterns. This paper develops a method of pattern discovery that models human judgement of what constitutes a significant pattern by incorporating annotations of repeated patterns, avoiding the need to design heuristics.  We take pattern discovery as a clustering task, where the input is a set of passages of monophonic music, represented as vectors of extracted features, and the output clusters correspond to discovered patterns. The human annotations are used to train a neural network to learn a low-dimensional embedding of the feature space that maps passages of music close together when they are occurrences of the same ground-truth pattern. The results of this approach match up with the annotations significantly better than the results of an approach using clustering without subspace learning. We provide examples of the types of patterns that this method tends to discover and discuss its feasibility and practicality as a tool for extracting useful information about repetitive structure in music.",
    "zenodo_id": 3527922,
    "dblp_key": null,
    "extra": {
      "takeaway": "Musical pattern discovery can be taken as a clustering task, incorporating manual annotations of repeated patterns as a way of specifying the kinds of patterns desired.",
      "external_links": [
        "https://github.com/timothydereuse/musical-pattern-clustering/blob/master/pattern_clustering_ismir_supplemental.pdf"
      ],
      "session_position": "F-15"
    }
  },
  {
    "title": "A Study of Annotation and Alignment Accuracy for Performance Comparison in Complex Orchestral Music",
    "author": [
      "Thassilo Gadermaier",
      "Gerhard Widmer"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527924",
    "url": "https://doi.org/10.5281/zenodo.3527924",
    "ee": "http://archives.ismir.net/ismir2019/paper/000094.pdf",
    "pages": "769-775",
    "abstract": "Quantitative analysis of commonalities and differences between recorded music performances is an increasingly common task in computational musicology. A typical scenario involves manual annotation of different recordings of the same piece along the time dimension, for comparative analysis of, e.g., the musical tempo, or for mapping other performance-related information between performances. This can be done by manually annotating one reference performance, and then automatically synchronizing other performances, using audio-to-audio alignment algorithms. In this paper we address several questions related to those tasks. First, we analyze different annotations of the same musical piece, quantifying timing deviations between the respective human annotators. A statistical evaluation of the marker time stamps will provide (a) an estimate of the expected timing precision of human annotations and (b) a ground truth for subsequent automatic alignment experiments. We then carry out a systematic evaluation of different audio features for audio-to-audio alignment, quantifying the degree of alignment accuracy that can be achieved, and relate this to the results from the annotation study.",
    "zenodo_id": 3527924,
    "dblp_key": null,
    "extra": {
      "takeaway": "Annotations of the \"beat\" of complex orchestral music have considerable uncertainty due to disagreement of annotators. A comparison of typical uncertainties to accuracies achieved by transfer of annotations using dynamic time-warping is given.",
      "external_links": [
        "https://doi.org/10.5281/zenodo.3260499"
      ],
      "session_position": "F-16"
    }
  },
  {
    "title": "Mapping Timing Strategies in Drum Performance",
    "author": [
      "George Sioros",
      "Guilherme C\u00e2mara",
      "Anne Danielsen"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527926",
    "url": "https://doi.org/10.5281/zenodo.3527926",
    "ee": "http://archives.ismir.net/ismir2019/paper/000095.pdf",
    "pages": "776-783",
    "abstract": "How do drummers express different timing styles? We conducted an experiment in which we asked twenty-two professional drummers to perform a simple rhythmic pattern while listening to a metronome. Here, we investigate the strategies they employed to express three different instructed timing profiles for the same pattern: \u201con\u201d, \u201cpushed\u201d and \u201claidback\". Our analysis of the recordings follows three stages. First, we compute sixteen binary features that capture the micro-timing relations of the kick, snare and hi-hat drum on-sets, between each other and with regards to the metrical grid. Second, we construct a micro-timing profile (mtP) for every performance by averaging the binary features across the recording. An mtP codifies the frequency with which the various features were found in a performance. Third, through a \u201csimilarity profiles\u201d hierarchical clustering analysis, we identify groups of recordings with significant similarities in their mtPs. We found distinct strategies to express each intended timing profile that employ specific combinations of relations between the instruments and with regards to the meter. Finally, we created a map that summarizes the main characteristics of the strategies and their relations using a phylogenetic tree visualization.",
    "zenodo_id": 3527926,
    "dblp_key": null,
    "extra": {
      "takeaway": "We present a novel method for the analysis and visualization of microtiming relations between instruments and apply it to drum performances with three different timing profiles (on, pushed and laidback) from a laboratory experiment.",
      "external_links": [],
      "session_position": "F-17"
    }
  },
  {
    "title": "Improving Singing Aid System for Laryngectomees With Statistical Voice Conversion and VAE-SPACE",
    "author": [
      "Li Li",
      "Tomoki Toda",
      "Kazuho Morikawa",
      "Kazuhiro Kobayashi",
      "Shoji Makino"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527928",
    "url": "https://doi.org/10.5281/zenodo.3527928",
    "ee": "http://archives.ismir.net/ismir2019/paper/000096.pdf",
    "pages": "784-790",
    "abstract": "This paper proposes an improved singing aid system for laryngectomees that converts electrolaryngeal (EL) speech produced using an electrolarynx to a more naturally sounding singing voice. Although the previously proposed system employing a noise suppression process and a rule-based pitch control approach has achieved preliminary success in converting EL speech into a singing voice, there are still two major limitations. First, the converted singing voice still sounds mechanical and unnatural owing to the adverse impacts of spectrograms extracted from EL speeches, also making the effect of pitch control limited. Second, the capability and flexibility of the rule-based pitch control in modeling various singing styles are insufficient, causing the converted singing voices to lack variety. To address these limitations, this paper proposes an improved system that uses 1) a statistical voice conversion approach to convert spectrograms extracted from EL speeches into those of natural speeches and 2) a deep generative model-based approach called VAE-SPACE for pitch modification, which generates pitch patterns in a data-driven manner instead of following manually designed rules. The experimental results revealed that 1) the conversion of spectrograms was effective in improving the naturalness of singing voices, and 2) the statistical pitch control approach was able to achieve comparable results with the rule-based approach, which was very carefully designed to be specialized in singing.",
    "zenodo_id": 3527928,
    "dblp_key": null,
    "extra": {
      "takeaway": "An improved singing aid system for laryngectomees is developed, which converts EL speeches into singing voices according to the melodic information by applying a statistical VC approach to enhance phonetic features and VAE-SPACE to control pitch. ",
      "external_links": [],
      "session_position": "F-18"
    }
  },
  {
    "title": "Approachable Music Composition with Machine Learning at Scale",
    "author": [
      "Cheng-Zhi Anna Huang",
      "Curtis Hawthorne",
      "Adam Roberts",
      "Monica  Dinculescu",
      "James Wexler",
      "Leon Hong",
      "Jacob Howcroft"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527930",
    "url": "https://doi.org/10.5281/zenodo.3527930",
    "ee": "http://archives.ismir.net/ismir2019/paper/000097.pdf",
    "pages": "793-800",
    "abstract": "To make music composition more approachable, we designed the first AI-powered Google Doodle, the Bach Doodle, where users can create their own melody and have it harmonized by a machine learning model Coconet (Huang et al., 2017) in the style of Bach.  For users to input melodies, we designed a simplified sheet-music based interface.  To support an interactive experience at scale, we re-implemented Coconet in TensorFlow.js (Smilkov et al., 2019) to run in the browser and reduced its runtime from 40s to 2s by adopting dilated depth-wise separable convolutions and fusing operations.  We also reduced the model download size to approximately 400KB through post-training weight quantization.  We calibrated a speed test based on partial model evaluation time to determine if the harmonization request should be performed locally or sent to remote TPU servers.  In three days, people spent 350 years worth of time playing with the Bach Doodle, and Coconet received more than 55 million queries.  Users could choose to rate their compositions and contribute them to a public dataset, which we are releasing with this paper.  We hope that the community finds this dataset useful for applications ranging from ethnomusicological studies, to music education, to improving machine learning models.",
    "zenodo_id": 3527930,
    "dblp_key": null,
    "extra": {
      "takeaway": "We show behind the scenes how the Bach Doodle works, the design, how we sped up the machine learning model Coconet to run in the browser.  We are also releasing a dataset of 21.6 million melody and harmonization pairs, along with user ratings.",
      "external_links": [],
      "session_position": "G-01"
    }
  },
  {
    "title": "Scalable Searching and Ranking  for Melodic Pattern Queries",
    "author": [
      "Philippe Rigaux",
      "Nicolas Travers"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527932",
    "url": "https://doi.org/10.5281/zenodo.3527932",
    "ee": "http://archives.ismir.net/ismir2019/paper/000098.pdf",
    "pages": "801-808",
    "abstract": "We present the design and implementation of a scalable search engine for large Digital Score Libraries. It covers the core features expected from an information retrieval system. Music representation is pre-processed, simplified and normalized. Collections are searched for scores that match a melodic pattern, results are ranked on their similarity with the pattern, and matching fragments are finally identified on the fly. Moreover, all these features are designed to be integrated in a standard search engine and thus benefit from the horizontal scalability of such systems. Our method is fully implemented, and relies on Elasticsearch for collection indexing. We describe its main components, report and study its performances.",
    "zenodo_id": 3527932,
    "dblp_key": null,
    "extra": {
      "takeaway": "We focus in this paper on the scalable content-based retrieval problem. We consider the search mechanism with a monophonic query pattern in order to retrieve from a very large collection of scores one or more fragments \"similar\" to this pattern.",
      "external_links": [],
      "session_position": "G-02"
    }
  },
  {
    "title": "Adaptive Time-Frequency Scattering for Periodic Modulation Recognition in Music Signals",
    "author": [
      "Changhong Wang",
      "Emmanouil Benetos",
      "Vincent Lostanlen",
      "Elaine Chew"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527934",
    "url": "https://doi.org/10.5281/zenodo.3527934",
    "ee": "http://archives.ismir.net/ismir2019/paper/000099.pdf",
    "pages": "809-815",
    "abstract": "Vibratos, tremolos, trills, and flutter-tongue are techniques frequently found in vocal and instrumental music. A common feature of these techniques is the periodic modulation in the time--frequency domain. We propose a representation based on time--frequency scattering to model the inter-class variability for fine discrimination of these periodic modulations. Time--frequency scattering is an instance of the scattering transform, an approach for building invariant, stable, and informative signal representations. The proposed representation is calculated around the wavelet subband of maximal acoustic energy, rather than over all the wavelet bands. To demonstrate the feasibility of this approach, we build a system that computes the representation as input to a machine learning classifier. Whereas previously published datasets for playing technique analysis focus primarily on techniques recorded in isolation, for ecological validity, we create a new dataset to evaluate the system. The dataset, named CBF-periDB, contains full-length expert performances on the Chinese bamboo flute that have been thoroughly annotated by the players themselves. We report F-measures of 99% for flutter-tongue, 82% for trill, 69% for vibrato, and 51% for tremolo detection, and provide explanatory visualisations of scattering coefficients for each of these techniques.",
    "zenodo_id": 3527934,
    "dblp_key": null,
    "extra": {
      "takeaway": "Scattering transform provides a versatile and compact representation for analysing playing techniques.",
      "external_links": [],
      "session_position": "G-03"
    }
  },
  {
    "title": "Controlling Symbolic Music Generation based on Concept Learning from Domain Knowledge",
    "author": [
      "Taketo Akama"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527936",
    "url": "https://doi.org/10.5281/zenodo.3527936",
    "ee": "http://archives.ismir.net/ismir2019/paper/000100.pdf",
    "pages": "816-823",
    "abstract": "Machine learning allows automatic construction of generative models for music. However, they are learned from only the succession of notes itself without explicitly employing domain knowledge of musical concepts such as rhythm, contour, and fragmentation & consolidation. We approximate such musical domain knowledge as a function, and feed it into our model. Then, two decoupled spaces are learned: the extraction space that captures the target concept, and the residual space that captures the remainder. For monophonic symbolic music, our model exhibits high decoupling/modeling performance.  Controllability in generation is improved: (i) our interpolation enables concept-aware flexible control over blending two musical fragments, and (ii) our variation generation enables users to make concept-aware adjustable variations.",
    "zenodo_id": 3527936,
    "dblp_key": null,
    "extra": {
      "takeaway": "ExtRes is a generative model that learns decoupled concept spaces, given human domain knowledge. It provides concept-aware (e.g., rhythm, contour) controllability in interpolation and variation generation for symbolic music.",
      "external_links": [],
      "session_position": "G-04"
    }
  },
  {
    "title": "Unmixer: An Interface for Extracting and Remixing Loops",
    "author": [
      "Jordan Smith",
      "Yuta Kawasaki",
      "Masataka Goto"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527938",
    "url": "https://doi.org/10.5281/zenodo.3527938",
    "ee": "http://archives.ismir.net/ismir2019/paper/000101.pdf",
    "pages": "824-831",
    "abstract": "To create their art, remix artists would like to have segmented stem tracks at their disposal; that is, isolated instances of the loops and sounds that the original composer used to create a track. We present Unmixer, a web service that will analyze and extract loops from any audio uploaded by a user. The loops are presented in an interface that allows users to immediately remix the loops; if users upload multiple tracks, they can easily create mash-ups with the loops, which are automatically matched in tempo. To analyze the audio, we use a recently-proposed method of source separation based on the nonnegative Tucker decomposition of the spectrum. To reduce interference among the extracted loops, we propose an extra factorization step with a sparseness constraint and demonstrate that it improves the source separation result. We also propose a method for selecting the best instances of the extracted loops and demonstrate its effectiveness in an evaluation. Both of these improvements are incorporated into the backend of the interface. Finally, we discuss the feedback collected in a set of user evaluations.",
    "zenodo_id": 3527938,
    "dblp_key": null,
    "extra": {
      "takeaway": "Unmixer is a web interface where users can upload music, extract loops, remix them, and mash-up loops from different songs. To extract loops with source separation, we use a nonnegative tensor factorization method improved with a sparsity constraint.",
      "external_links": [],
      "session_position": "G-05"
    }
  },
  {
    "title": "Quantifying Disruptive Influence in the AllMusic Guide",
    "author": [
      "Flavio Figueiredo",
      "Nazareno Andrade"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527940",
    "url": "https://doi.org/10.5281/zenodo.3527940",
    "ee": "http://archives.ismir.net/ismir2019/paper/000102.pdf",
    "pages": "832-838",
    "abstract": "Understanding how influences shape musical creation provides rich insight into cultural trends. As such, there have been several efforts to create quantitative complex network methods that support the analysis of influence networks among artists in a music corpus. We contribute to this body of work by examining how disruption happens in a corpus about music influence from the All Music Guide. A disruptive artist is one that creates a new stream of influences; this artist builds on prior efforts, but influences subsequent artists that do not build on the same prior efforts. We leverage methods devised to study disruption in Science and Technology, and apply them to the context of music creation. Our results point that such methods identify innovative artists, and that disruption is often uncorrelated with network centrality.",
    "zenodo_id": 3527940,
    "dblp_key": null,
    "extra": {
      "takeaway": "What is disruption? Different from being popular, being disruptive usually means bringing something ground-breaking to the table. In this work, we measure and detail how artists are disruptive using a human-curated music corpora.",
      "external_links": [
        "https://github.com/flaviovdf/allmusic-disruption"
      ],
      "session_position": "G-06"
    }
  },
  {
    "title": "Leveraging knowledge bases and parallel annotations for music genre translation",
    "author": [
      "Elena Epure",
      "Anis Khlif",
      "Romain Hennequin"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527944",
    "url": "https://doi.org/10.5281/zenodo.3527944",
    "ee": "http://archives.ismir.net/ismir2019/paper/000103.pdf",
    "pages": "839-846",
    "abstract": "Prevalent efforts have been put in automatically inferring genres of musical items. Yet, the propose solutions often rely on simplifications and fail to address the diversity and subjectivity of music genres. Accounting for these has, though, many benefits for aligning knowledge sources, integrating data and enriching musical items with tags. Here, we choose a new angle for the genre study by seeking to predict what would be the genres of musical items in a target tag system, knowing the genres assigned to them within source tag systems. We call this a translation task and identify three cases: 1) no common annotated corpus between source and target tag systems exists, 2) such a large corpus exists, 3) only few common annotations exist. We propose the related solutions: a knowledge-based translation modeled as taxonomy mapping, a statistical translation modeled with maximum likelihood logistic regression; a hybrid translation modeled with maximum a posteriori logistic regression with priors given by the knowledge-based translation. During evaluation, the solutions fit well the identified cases and the hybrid translation is systematically the most effective w.r.t. multilabel  classification metrics. This is a first attempt to unify genre tag systems by leveraging both representation and interpretation diversity.",
    "zenodo_id": 3527944,
    "dblp_key": null,
    "extra": {
      "takeaway": "In this paper, we explore the problem of translation of music genres between multiple tag systems, with or without common annotated corpus.",
      "external_links": [],
      "session_position": "G-07"
    }
  },
  {
    "title": "Generating Structured Drum Pattern Using Variational Autoencoder and Self-similarity Matrix",
    "author": [
      "I-Chieh Wei",
      "Chih-Wei Wu",
      "Li Su"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527946",
    "url": "https://doi.org/10.5281/zenodo.3527946",
    "ee": "http://archives.ismir.net/ismir2019/paper/000104.pdf",
    "pages": "847-854",
    "abstract": "Drum pattern generation is a task that focuses on the rhythmic aspect of music and aims at generating percussive sequences. With the advancement of machine learning techniques, several models have been proven useful in producing compelling results. However, one of the main challenges is to generate structurally cohesive sequences. In this study, a drum pattern generation model based on Variational Autoencoders (VAEs) is presented; Specifically, the proposed model is built to generate symbolic drum patterns given an accompaniment that consists of melodic sequences. A self-similarity matrix (SSM) is incorporated in the process for encapsulating structural information. Both the objective evaluation and the subjective listening test highlight the model\u2019s capability of creating musically meaningful transitions on structural boundaries.",
    "zenodo_id": 3527946,
    "dblp_key": null,
    "extra": {
      "takeaway": "A drum pattern generation model based on VAE-GAN is presented; the proposed method generates symbolic drum patterns given a melodic track. Self-similarity matrix (SSM) is incorporated in the  process  for  encapsulating  structural  information.",
      "external_links": [],
      "session_position": "G-08"
    }
  },
  {
    "title": "Rendering Music Performance With Interpretation Variations Using Conditional Variational RNN",
    "author": [
      "Akira Maezawa",
      "Kazuhiko Yamamoto",
      "Takuya Fujishima"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527948",
    "url": "https://doi.org/10.5281/zenodo.3527948",
    "ee": "http://archives.ismir.net/ismir2019/paper/000105.pdf",
    "pages": "855-861",
    "abstract": "Capturing and generating a wide variety of musical expression is important in music performance rendering, but current methods fail to model such a variation.  This paper presents a music performance rendering method that could explicitly model differences in interpretations for a given piece of music.  Conditional variational auto-encoder is used to jointly train, conditioned on the music score, an encoder from performance to a latent code and a decoder from the latent code to music performance.  Evaluation demonstrates the method is capable of generating a wide variety of human-like expressive music performances as the latent code is varied.",
    "zenodo_id": 3527948,
    "dblp_key": null,
    "extra": {
      "takeaway": "Our performance rendering method discovers latent sources of expressive variety, and also allows users to control such sources of expressive variations when rendering.",
      "external_links": [
        "https://sites.google.com/view/cvrnn-performance-render"
      ],
      "session_position": "G-09"
    }
  },
  {
    "title": "An Interactive Workflow for Generating Chord Labels for Homorhythmic Music in Symbolic Formats",
    "author": [
      "Yaolong Ju",
      "Samuel Howes",
      "Cory McKay",
      "Nathaniel Condit-Schultz",
      "Jorge Calvo-Zaragoza",
      "Ichiro Fujinaga"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527950",
    "url": "https://doi.org/10.5281/zenodo.3527950",
    "ee": "http://archives.ismir.net/ismir2019/paper/000106.pdf",
    "pages": "862-869",
    "abstract": "Automatic harmonic analysis is challenging: rule-based models cannot account for every possible edge case, and manual annotation is expensive and sometimes inconsistent, undermining the training and evaluation of machine learning models. We present an interactive workflow to address these problems, and test it on Bach chorales. First, a rule-based model was used to generate preliminary, consistent chord labels in order to pre-train three machine learning models. These four models were grouped into an ensemble that generated chord labels by voting, achieving 91.4% accuracy on a reserved test set. A domain expert then corrected only those chords that the ensemble did not agree on unanimously (20.9% of the generated labels). Finally, we used these corrected annotations to re-train the machine learning models, and the resulting ensemble attained an accuracy of 93.5% on the reserved test set, a 24.4% reduction in the number of errors. This versatile interactive workflow can either work in a fully automatic way, or can capitalize on relatively minimal human involvement to generate higher-quality chord labels. It combines the consistency of rule-based models with the nuance of manual analysis to generate relatively inexpensive high-quality ground truth for training effective machine learning models.",
    "zenodo_id": 3527950,
    "dblp_key": null,
    "extra": {
      "takeaway": "An Interactive Workflow for Generating Chord Labels for Homorhythmic Music in Symbolic Formats",
      "external_links": [],
      "session_position": "G-10"
    }
  },
  {
    "title": "Quantifying Musical Style: Ranking Symbolic Music based on Similarity to a Style",
    "author": [
      "Jeffrey Ens",
      "Philippe Pasquier"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527952",
    "url": "https://doi.org/10.5281/zenodo.3527952",
    "ee": "http://archives.ismir.net/ismir2019/paper/000107.pdf",
    "pages": "870-877",
    "abstract": "Modelling human perception of musical similarity is critical for the evaluation of generative music systems, musicological research, and many Music Information Retrieval tasks. Although human similarity judgments are the gold standard, computational analysis is often preferable, since results are often easier to reproduce, and computational methods are much more scalable. Moreover, computation based approaches can be calculated quickly and on demand, which is a prerequisite for use with an online system. We propose StyleRank, a method to measure the similarity between a MIDI file and an arbitrary musical style delineated by a collection of MIDI files. MIDI files are encoded using a novel set of features and an embedding is learned using Random Forests. Experimental evidence demonstrates that StyleRank is highly correlated with human perception of stylistic similarity, and that it is precise enough to rank generated samples based on their similarity to the style of a corpus. In addition, similarity can be measured with respect to a single feature, allowing specific discrepancies between generated samples and a particular musical style to be identified.",
    "zenodo_id": 3527952,
    "dblp_key": null,
    "extra": {
      "takeaway": "StyleRank is a method to rank MIDI files based on their similarity to a style defined by an arbitrary corpus.",
      "external_links": [
        "https://github.com/jeffreyjohnens/style_rank"
      ],
      "session_position": "G-11"
    }
  },
  {
    "title": "Audio Query-based Music Source Separation",
    "author": [
      "Jie Hwan Lee",
      "Hyeong-Seok Choi",
      "Kyogu Lee"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527954",
    "url": "https://doi.org/10.5281/zenodo.3527954",
    "ee": "http://archives.ismir.net/ismir2019/paper/000108.pdf",
    "pages": "878-885",
    "abstract": "In recent years, music source separation has been one of the most intensively studied research areas in music information retrieval. Improvements in deep learning lead to a big progress in music source separation performance. However, most of the previous studies are restricted to separating a few limited number of sources, such as vocals, drums, bass, and other. In this study, we propose a network for audio query-based music source separation that can explicitly encode the source information from a query signal regardless of the number and/or kind of target signals. The proposed method consists of a Query-net and a Separator: given a query and a mixture, the Query-net encodes the query into the latent space, and the Separator estimates masks conditioned by the latent vector, which is then applied to the mixture for separation. The Separator can also generate masks using the latent vector from the training samples, allowing separation in the absence of a query. We evaluate our method on the MUSDB18 dataset, and experimental results show that the proposed method can separate multiple sources with a single network. In addition, through further investigation of the latent space we demonstrate that our method can generate continuous outputs via latent vector interpolation.",
    "zenodo_id": 3527954,
    "dblp_key": null,
    "extra": {
      "takeaway": "An audio-query based source separation method that is capable of separating the music source regardless of the number and/or kind of target signals. Various useful scenarios are suggested such as zero-shot separation, latent interpolation and etc.",
      "external_links": [],
      "session_position": "G-12"
    }
  },
  {
    "title": "Mosaic Style Transfer Using Sparse Autocorrelograms",
    "author": [
      "Daniel MacKinlay",
      "Zdravko Botev"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527956",
    "url": "https://doi.org/10.5281/zenodo.3527956",
    "ee": "http://archives.ismir.net/ismir2019/paper/000109.pdf",
    "pages": "886-893",
    "abstract": "We introduce a novel mosaic synthesis algorithm for musical style transfer using the autocorrelogram as a feature map. We decompose the autocorrelogram feature map sparsely in a decaying sinusoid basis, using that decomposition as an interpolation scheme in feature space. This efficiently provides gradient information in the mosaicing optimization, including of the challenging time-scale parameters which are usually intractable for discretely sampled signals. The required calculations are straightforward to parallelize on vector-processing hardware. Our implementation of the method provides good quality output and novel musical effects in example tasks by itself and can also be integrated into alternative mosaicing methods.",
    "zenodo_id": 3527956,
    "dblp_key": null,
    "extra": {
      "takeaway": "We apply sparse dictionary decomposition twice to autocorrelograms of signals, to get a novel analysis of and method for mosaicing music style transfer, which has the novel feature of handling time-scaling of the source audio naturally.",
      "external_links": [
        "https://github.com/danmackinlay/mosaicing_omp_ismir_2019/"
      ],
      "session_position": "G-13"
    }
  },
  {
    "title": "Automatic Choreography Generation with Convolutional Encoder-decoder Network",
    "author": [
      "Juheon Lee",
      "Seohyun Kim",
      "Kyogu Lee"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527958",
    "url": "https://doi.org/10.5281/zenodo.3527958",
    "ee": "http://archives.ismir.net/ismir2019/paper/000110.pdf",
    "pages": "894-899",
    "abstract": "Automatic choreography generation is a challenging task because it often requires an understanding of two abstract concepts - music and dance - which are realized in the two different modalities, namely audio and video, respectively. In this paper, we propose a music-driven choreography generation system using an auto-regressive encoder-decoder network. To this end, we first collect a set of multimedia clips that include both music and corresponding dance motion. We then extract the joint coordinates of the dancer from video and the mel-spectrogram of music from audio and train our network using music-choreography pairs as input. Finally, a novel dance motion is generated at the inference time when only music is given as an input. We performed a user study for a qualitative evaluation of the proposed method, and the results show that the proposed model is able to generate musically meaningful and natural dance movements given an unheard song.",
    "zenodo_id": 3527958,
    "dblp_key": null,
    "extra": {
      "takeaway": "In this paper, we proposed an encoder-decoder neural network that generates choreography that matches with given music. As a result of the evaluation, we showed that the proposed network created a natural choreography that matched the music.",
      "external_links": [
        "http://listentodance.strikingly.com/"
      ],
      "session_position": "G-14"
    }
  },
  {
    "title": "Hierarchical Classification Networks for Singing Voice Segmentation and Transcription",
    "author": [
      "Fu Zih-Sing",
      "Li Su"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527960",
    "url": "https://doi.org/10.5281/zenodo.3527960",
    "ee": "http://archives.ismir.net/ismir2019/paper/000111.pdf",
    "pages": "900-907",
    "abstract": "Identifying the onset and offset time of a musical note is a challenging step for singing voice transcription, as the soft onset/offset, portamento, and vibrato phenomena are rich in singing voice signals. In this work, we investigate how to utilize local data representation with pattern recognition for onset and offset detection of singing voice. We consider onset and offset detection as a hierarchical classification problem, where every local data representation as input is classified into one of all the possible event states in monophonic singing, namely the silence, activation, and transition states, and the transition state is further classified into the onset and offset states. An objective function based on this hierarchical taxonomy nicely guides the model to capture the complicated temporal dynamics of note sequences. Multi-channel data representations containing spectral differences and pitch saliency are employed to reflect the patterns of note transition in singing voice signals. The proposed method implemented with residual networks provides improved performance over prior art in onset and offset detection. Moreover, by integrating with a pitch detection framework, the proposed method also outperforms previous singing voice transcription methods. This result emphasizes the importance of note segmentation in singing voice transcription.",
    "zenodo_id": 3527960,
    "dblp_key": null,
    "extra": {
      "takeaway": "A note transcription method for singing voice, implemented by novel hierarchical classification networks, achieves the performance better than before.",
      "external_links": [
        "https://github.com/Itachi6912110/Hierarchical-Note-Segmentation"
      ],
      "session_position": "G-15"
    }
  },
  {
    "title": "VirtuosoNet: A Hierarchical RNN-based System for Modeling Expressive Piano Performance",
    "author": [
      "Dasaem Jeong",
      "Taegyun Kwon",
      "Yoojin Kim",
      "Kyogu Lee",
      "Juhan Nam"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527962",
    "url": "https://doi.org/10.5281/zenodo.3527962",
    "ee": "http://archives.ismir.net/ismir2019/paper/000112.pdf",
    "pages": "908-915",
    "abstract": "In this paper, we present our application of deep neural network to modeling piano performance, which imitates the expressive control of tempo, dynamics, articulations and pedaling from pianists. Our model consists of recurrent neural networks with hierarchical attention and conditional variational autoencoder. The model takes a sequence of note-level score features extracted from MusicXML as input and predicts piano performance features of the corresponding notes. To render musical expressions consistently over long-term sections, we first predict tempo and dynamics in measure-level and, based on the result, refine them in note-level. The evaluation through listening test shows that our model achieves a more human-like expressiveness compared to previous models. We also share the dataset we used for the experiment.    ",
    "zenodo_id": 3527962,
    "dblp_key": null,
    "extra": {
      "takeaway": "We present an RNN-based model that reads MusicXML and generates human-like performance MIDI. The model employs a hierarchical approach by using attention network and an independent measure-level estimation module. We share our code and dataset.",
      "external_links": [
        "https://github.com/jdasam/virtuosoNet"
      ],
      "session_position": "G-16"
    }
  },
  {
    "title": "MIDI Passage Retrieval Using Cell Phone Pictures of Sheet Music",
    "author": [
      "Daniel Yang",
      "Thitaree Tanprasert",
      "Teerapat Jenrungrot",
      "Mengyi Shan",
      "Timothy Tsai"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527964",
    "url": "https://doi.org/10.5281/zenodo.3527964",
    "ee": "http://archives.ismir.net/ismir2019/paper/000113.pdf",
    "pages": "916-923",
    "abstract": "This paper investigates a cross-modal retrieval problem in which a user would like to retrieve a passage of music from a MIDI file by taking a cell phone picture of a physical page of sheet music.  While audio-sheet music retrieval has been explored by a number of works, this scenario is novel in that the query is a cell phone picture rather than a digital scan.  To solve this problem, we introduce a mid-level feature representation called a bootleg score which explicitly encodes the rules of Western musical notation.  We convert both the MIDI and the sheet music into bootleg scores using deterministic rules of music and classical computer vision techniques for detecting simple geometric shapes.  Once the MIDI and cell phone image have been converted into bootleg scores, we estimate the alignment using dynamic programming.  The most notable characteristic of our system is that it does test-time adaptation and has no trainable weights at all--only a set of about 30 hyperparameters.  On a dataset containing 1000 cell phone pictures taken of 100 scores of classical piano music, our system achieves an F measure score of .869 and outperforms baseline systems based on commercial optical music recognition software. ",
    "zenodo_id": 3527964,
    "dblp_key": null,
    "extra": {
      "takeaway": "We develop a system which enables a person to take a cell phone picture of a page of sheet music, and to automatically retrieve the matching portion of a corresponding MIDI file.",
      "external_links": [],
      "session_position": "G-17"
    }
  },
  {
    "title": "A Convolutional Approach to Melody Line Identification in Symbolic Scores",
    "author": [
      "Federico Simonetta",
      "Carlos Eduardo Cancino-Chac\u00f3n",
      "Stavros Ntalampiras",
      "Gerhard Widmer"
    ],
    "year": "2019",
    "doi": "10.5281/zenodo.3527966",
    "url": "https://doi.org/10.5281/zenodo.3527966",
    "ee": "http://archives.ismir.net/ismir2019/paper/000114.pdf",
    "pages": "924-931",
    "abstract": "In many musical traditions, the melody line is of primary significance in a piece. Human listeners can readily distinguish melodies from accompaniment; however, making this distinction given only the written score \u2013 i.e. without listening to the music performed \u2013 can be a difficult task. Solving this task is of great importance for both Music Information Retrieval and musicological applications.  In this paper, we propose an automated approach to identifying the most salient melody line in a symbolic score. The backbone of the method consists of a convolutional neural network (CNN) estimating the probability that each note in the score (more precisely: each pixel in a piano roll encoding of the score) belongs to the melody line. We train and evaluate the method on various datasets, using manual annotations where available and solo instrument parts where not. We also propose a method to inspect the CNN and to analyze the influence exerted by notes on the prediction of other notes; this method can be applied whenever the output of a neural network has the same size as the input.",
    "zenodo_id": 3527966,
    "dblp_key": null,
    "extra": {
      "takeaway": "We propose a new approach to identifying the most salient melody line in a symbolic score, consisting of a CNN estimating the probability that each note in the score belongs to the melody. This task is important for both MIR and Musicology.",
      "external_links": [
        "https://limunimi.github.io/Symbolic-Melody-Identification/"
      ],
      "session_position": "G-18"
    }
  }
]
